{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync Validations Excel\n",
    "\n",
    "Synchronisation bidirectionnelle entre Excel et Fabric pour la validation des opportunités.\n",
    "\n",
    "## Modes d'exécution\n",
    "\n",
    "| Mode | Description | Fréquence |\n",
    "|------|-------------|-----------|\n",
    "| `export` | Lakehouse → Excel (MERGE: ajoute nouvelles opportunités) | Hebdomadaire |\n",
    "| `import` | Excel → Lakehouse + SQL + envoi emails | Quotidien |\n",
    "| `full_sync` | Export + Import | Ad hoc |\n",
    "\n",
    "## Fichier Excel\n",
    "\n",
    "- **Emplacement** : `/lakehouse/default/Files/weak_signals_validation.xlsx`\n",
    "- **Accès users** : OneLake File Explorer ou téléchargement depuis Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Mode d'exécution: \"export\", \"import\", \"full_sync\"\n",
    "MODE = \"full_sync\"\n",
    "\n",
    "# Semaine d'ingestion à exporter (format: \"2024-W49\")\n",
    "# None = semaine courante automatique\n",
    "INGESTION_WEEK = None\n",
    "\n",
    "# Chemin du fichier Excel dans le Lakehouse\n",
    "EXCEL_PATH = \"/lakehouse/default/Files/weak_signals_validation.xlsx\"\n",
    "\n",
    "# Tables Lakehouse\n",
    "TABLE_OPPORTUNITIES = \"landing_feedly_opportunities\"\n",
    "TABLE_SALERS = \"landing_salers\"\n",
    "TABLE_SALERS_BD = \"landing_salers_bd\"\n",
    "TABLE_VALIDATIONS = \"landing_validations\"\n",
    "\n",
    "# Configuration SMTP (pour les emails)\n",
    "SMTP_ENABLED = True  # Mettre à False pour désactiver les emails\n",
    "SMTP_HOST = \"smtp.office365.com\"\n",
    "SMTP_PORT = 587\n",
    "SMTP_USER = \"\"  # À remplir: votre email @l-acoustics.com\n",
    "SMTP_PASSWORD = \"\"  # À remplir: App Password\n",
    "\n",
    "# Calculer la semaine courante si non spécifiée\n",
    "from datetime import datetime\n",
    "if INGESTION_WEEK is None:\n",
    "    INGESTION_WEEK = datetime.now().strftime(\"%Y-W%V\")\n",
    "\n",
    "print(f\"Mode: {MODE}\")\n",
    "print(f\"Semaine d'ingestion: {INGESTION_WEEK}\")\n",
    "print(f\"Excel: {EXCEL_PATH}\")\n",
    "print(f\"SMTP: {'Activé' if SMTP_ENABLED else 'Désactivé'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports et Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, lit, trim, lower, when, coalesce\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark session OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema de la table landing_validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema pour les validations dans le Lakehouse\n",
    "schema_validations = StructType([\n",
    "    # Identifiant\n",
    "    StructField(\"opportunity_id\", StringType(), False),\n",
    "    \n",
    "    # Semaine d'ingestion\n",
    "    StructField(\"ingestion_week\", StringType(), True),\n",
    "    \n",
    "    # Infos opportunité (pour contexte)\n",
    "    StructField(\"article_title\", StringType(), True),\n",
    "    StructField(\"article_url\", StringType(), True),\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"vertical\", StringType(), True),\n",
    "    StructField(\"evaluation_score\", IntegerType(), True),\n",
    "    StructField(\"audit_opportunity_reason\", StringType(), True),\n",
    "    \n",
    "    # Saler assigné\n",
    "    StructField(\"saler_name\", StringType(), True),\n",
    "    StructField(\"saler_email\", StringType(), True),\n",
    "    \n",
    "    # Déduplication (info contextuelle)\n",
    "    StructField(\"is_duplicate\", IntegerType(), True),  # 1=doublon, 0=unique\n",
    "    StructField(\"is_suspected_duplicate\", IntegerType(), True),  # 1=zone grise\n",
    "    StructField(\"duplicate_score\", DoubleType(), True),  # Score similarité\n",
    "    \n",
    "    # Validation (rempli par user)\n",
    "    StructField(\"is_validated\", IntegerType(), True),  # 1=OK, 0=KO, NULL=PENDING\n",
    "    StructField(\"validation_comment\", StringType(), True),\n",
    "    StructField(\"validated_by\", StringType(), True),\n",
    "    StructField(\"validation_date\", TimestampType(), True),\n",
    "    \n",
    "    # Email\n",
    "    StructField(\"email_sent_at\", TimestampType(), True),\n",
    "    \n",
    "    # Metadata\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"updated_at\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "print(f\"Schema landing_validations: {len(schema_validations.fields)} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "    \"\"\"Vérifie si un fichier existe dans le Lakehouse.\"\"\"\n",
    "    try:\n",
    "        mssparkutils.fs.head(path, 1)\n",
    "        return True\n",
    "    except:\n",
    "        return os.path.exists(path)\n",
    "\n",
    "\n",
    "def get_opportunities_with_salers(ingestion_week=None):\n",
    "    \"\"\"\n",
    "    Récupère les opportunités (audit_opportunity=1) avec les salers assignés.\n",
    "    \n",
    "    Filtres:\n",
    "    - audit_opportunity = 1\n",
    "    - is_duplicate = false OU NULL (exclut les doublons confirmés)\n",
    "    - ingestion_week = semaine spécifiée (si fournie)\n",
    "    \n",
    "    Jointure sur country pour RAE/RSM.\n",
    "    \"\"\"\n",
    "    week_filter = \"\"\n",
    "    if ingestion_week:\n",
    "        week_filter = f\"AND o.ingestion_week = '{ingestion_week}'\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        o.id,\n",
    "        o.ingestion_week,\n",
    "        o.article_title,\n",
    "        o.article_url,\n",
    "        o.venue_name,\n",
    "        o.city,\n",
    "        o.country,\n",
    "        o.vertical,\n",
    "        o.evaluation_score,\n",
    "        o.audit_opportunity_reason,\n",
    "        -- Déduplication\n",
    "        o.is_duplicate,\n",
    "        o.is_suspected_duplicate,\n",
    "        o.duplicate_score,\n",
    "        -- Saler\n",
    "        s.who AS saler_name,\n",
    "        s.email AS saler_email\n",
    "    FROM {TABLE_OPPORTUNITIES} o\n",
    "    LEFT JOIN {TABLE_SALERS} s \n",
    "        ON TRIM(LOWER(o.country)) = TRIM(LOWER(s.country))\n",
    "    WHERE o.audit_opportunity = 1\n",
    "      AND (o.is_duplicate = false OR o.is_duplicate IS NULL)\n",
    "      {week_filter}\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "\n",
    "def send_email(to, subject, body):\n",
    "    \"\"\"\n",
    "    Envoie un email via SMTP Office 365.\n",
    "    \"\"\"\n",
    "    if not SMTP_ENABLED:\n",
    "        print(f\"  [SMTP désactivé] Email non envoyé à {to}\")\n",
    "        return False\n",
    "    \n",
    "    if not SMTP_USER or not SMTP_PASSWORD:\n",
    "        print(f\"  [SMTP non configuré] Email non envoyé à {to}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        import smtplib\n",
    "        from email.mime.text import MIMEText\n",
    "        from email.mime.multipart import MIMEMultipart\n",
    "        \n",
    "        msg = MIMEMultipart(\"alternative\")\n",
    "        msg[\"Subject\"] = subject\n",
    "        msg[\"From\"] = SMTP_USER\n",
    "        msg[\"To\"] = to\n",
    "        \n",
    "        # Version HTML\n",
    "        html_part = MIMEText(body, \"html\")\n",
    "        msg.attach(html_part)\n",
    "        \n",
    "        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(SMTP_USER, SMTP_PASSWORD)\n",
    "            server.sendmail(SMTP_USER, to, msg.as_string())\n",
    "        \n",
    "        print(f\"  ✓ Email envoyé à {to}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Erreur envoi email à {to}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def format_validation_email(row):\n",
    "    \"\"\"\n",
    "    Formate le contenu de l'email de notification.\n",
    "    \"\"\"\n",
    "    status = \"OK (Validée)\" if row[\"is_validated\"] == 1 else \"KO (Rejetée)\"\n",
    "    status_color = \"#28a745\" if row[\"is_validated\"] == 1 else \"#dc3545\"\n",
    "    \n",
    "    # Extraire les valeurs pour éviter les backslashes dans le f-string\n",
    "    venue_name = row.get(\"venue_name\", \"N/A\")\n",
    "    city = row.get(\"city\", \"N/A\")\n",
    "    country = row.get(\"country\", \"N/A\")\n",
    "    vertical = row.get(\"vertical\", \"N/A\")\n",
    "    score = row.get(\"evaluation_score\", \"N/A\")\n",
    "    article_url = row.get(\"article_url\", \"#\")\n",
    "    article_title = row.get(\"article_title\", \"Voir l'article\")\n",
    "    audit_reason = row.get(\"audit_opportunity_reason\", \"N/A\")\n",
    "    validated_by = row.get(\"validated_by\", \"N/A\")\n",
    "    comment = row.get(\"validation_comment\", \"-\")\n",
    "    ingestion_week = row.get(\"ingestion_week\", \"N/A\")\n",
    "    \n",
    "    # Info déduplication si zone grise\n",
    "    dedup_info = \"\"\n",
    "    if row.get(\"is_suspected_duplicate\") == 1:\n",
    "        dedup_score = row.get(\"duplicate_score\", 0)\n",
    "        dedup_info = f\"\"\"\n",
    "            <div style=\"background: #fff3cd; padding: 10px; border-radius: 5px; margin-top: 10px;\">\n",
    "                <strong>⚠️ Zone grise:</strong> Cet article pourrait être un doublon (score: {dedup_score:.0%})\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <body style=\"font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;\">\n",
    "        <div style=\"background: {status_color}; color: white; padding: 20px; text-align: center;\">\n",
    "            <h1 style=\"margin: 0;\">Opportunité {status}</h1>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"padding: 20px; background: #f8f9fa;\">\n",
    "            <h2 style=\"color: #333; margin-top: 0;\">{venue_name}</h2>\n",
    "            <p><strong>Lieu:</strong> {city}, {country}</p>\n",
    "            <p><strong>Verticale:</strong> {vertical}</p>\n",
    "            <p><strong>Score IA:</strong> {score}/100</p>\n",
    "            <p><strong>Semaine:</strong> {ingestion_week}</p>\n",
    "            {dedup_info}\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"padding: 20px;\">\n",
    "            <h3>Article</h3>\n",
    "            <p><a href=\"{article_url}\">{article_title}</a></p>\n",
    "            \n",
    "            <h3>Justification IA</h3>\n",
    "            <p style=\"background: #e9ecef; padding: 10px; border-radius: 5px;\">\n",
    "                {audit_reason}\n",
    "            </p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"padding: 20px; background: #f8f9fa; border-top: 1px solid #ddd;\">\n",
    "            <p><strong>Validé par:</strong> {validated_by}</p>\n",
    "            <p><strong>Commentaire:</strong> {comment}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"padding: 10px; text-align: center; color: #666; font-size: 12px;\">\n",
    "            <p>Weak Signals Pipeline - L-Acoustics</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "\n",
    "print(\"Fonctions utilitaires chargées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODE EXPORT: Lakehouse → Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_export():\n",
    "    \"\"\"\n",
    "    Export les opportunités vers Excel.\n",
    "    \n",
    "    Filtres appliqués:\n",
    "    - ingestion_week = semaine courante (INGESTION_WEEK)\n",
    "    - is_duplicate = false (exclut les doublons confirmés)\n",
    "    - audit_opportunity = 1\n",
    "    \n",
    "    MERGE intelligent: ajoute les nouvelles, garde les validations existantes.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODE EXPORT: Lakehouse → Excel\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Semaine d'ingestion: {INGESTION_WEEK}\")\n",
    "    \n",
    "    # 1. Récupérer les opportunités avec salers (filtrées)\n",
    "    print(\"\\n1. Récupération des opportunités...\")\n",
    "    df_opportunities = get_opportunities_with_salers(ingestion_week=INGESTION_WEEK).toPandas()\n",
    "    print(f\"   {len(df_opportunities)} opportunités trouvées pour {INGESTION_WEEK}\")\n",
    "    \n",
    "    if len(df_opportunities) == 0:\n",
    "        print(\"   Aucune opportunité à exporter pour cette semaine.\")\n",
    "        return\n",
    "    \n",
    "    # Stats déduplication\n",
    "    n_suspected = len(df_opportunities[df_opportunities[\"is_suspected_duplicate\"] == True])\n",
    "    if n_suspected > 0:\n",
    "        print(f\"   ⚠️ {n_suspected} articles en zone grise (doublons potentiels)\")\n",
    "    \n",
    "    # 2. Préparer le DataFrame avec colonnes validation vides\n",
    "    df_new = df_opportunities.rename(columns={\"id\": \"opportunity_id\"})\n",
    "    df_new[\"is_validated\"] = None\n",
    "    df_new[\"validation_comment\"] = None\n",
    "    df_new[\"validated_by\"] = None\n",
    "    df_new[\"email_sent_at\"] = None\n",
    "    \n",
    "    # 3. Charger l'Excel existant (si existe)\n",
    "    print(\"\\n2. Vérification Excel existant...\")\n",
    "    if file_exists(EXCEL_PATH):\n",
    "        print(f\"   Fichier existant trouvé: {EXCEL_PATH}\")\n",
    "        df_existing = pd.read_excel(EXCEL_PATH)\n",
    "        print(f\"   {len(df_existing)} lignes existantes\")\n",
    "        \n",
    "        # 4. MERGE: garder les validations existantes\n",
    "        print(\"\\n3. MERGE...\")\n",
    "        \n",
    "        # IDs existants avec validation\n",
    "        existing_ids = set(df_existing[\"opportunity_id\"].dropna().astype(str))\n",
    "        \n",
    "        # Nouveaux IDs\n",
    "        new_ids = set(df_new[\"opportunity_id\"].astype(str))\n",
    "        \n",
    "        # IDs à ajouter (dans new mais pas dans existing)\n",
    "        ids_to_add = new_ids - existing_ids\n",
    "        print(f\"   IDs existants: {len(existing_ids)}\")\n",
    "        print(f\"   Nouveaux IDs: {len(ids_to_add)}\")\n",
    "        \n",
    "        # Filtrer les nouvelles lignes\n",
    "        df_to_add = df_new[df_new[\"opportunity_id\"].astype(str).isin(ids_to_add)]\n",
    "        \n",
    "        # Concaténer: existant + nouveaux\n",
    "        df_final = pd.concat([df_existing, df_to_add], ignore_index=True)\n",
    "        \n",
    "        print(f\"   Lignes ajoutées: {len(df_to_add)}\")\n",
    "        print(f\"   Total final: {len(df_final)}\")\n",
    "    else:\n",
    "        print(\"   Aucun fichier existant, création...\")\n",
    "        df_final = df_new\n",
    "    \n",
    "    # 5. Écrire le fichier Excel\n",
    "    print(\"\\n4. Écriture Excel...\")\n",
    "    \n",
    "    # Ordre des colonnes\n",
    "    columns_order = [\n",
    "        \"opportunity_id\",\n",
    "        \"ingestion_week\",\n",
    "        \"article_title\",\n",
    "        \"article_url\",\n",
    "        \"venue_name\",\n",
    "        \"city\",\n",
    "        \"country\",\n",
    "        \"vertical\",\n",
    "        \"evaluation_score\",\n",
    "        \"audit_opportunity_reason\",\n",
    "        \"is_duplicate\",\n",
    "        \"is_suspected_duplicate\",\n",
    "        \"duplicate_score\",\n",
    "        \"saler_name\",\n",
    "        \"saler_email\",\n",
    "        \"is_validated\",\n",
    "        \"validation_comment\",\n",
    "        \"validated_by\",\n",
    "        \"email_sent_at\"\n",
    "    ]\n",
    "    \n",
    "    # S'assurer que toutes les colonnes existent\n",
    "    for c in columns_order:\n",
    "        if c not in df_final.columns:\n",
    "            df_final[c] = None\n",
    "    \n",
    "    df_final = df_final[columns_order]\n",
    "    df_final.to_excel(EXCEL_PATH, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Fichier écrit: {EXCEL_PATH}\")\n",
    "    print(f\"   ✓ {len(df_final)} lignes totales\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "# Exécuter si mode export ou full_sync\n",
    "if MODE in [\"export\", \"full_sync\"]:\n",
    "    df_exported = run_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MODE IMPORT: Excel → Lakehouse + SQL + Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_import():\n",
    "    \"\"\"\n",
    "    Import les validations depuis Excel.\n",
    "    - MERGE dans Lakehouse (landing_validations)\n",
    "    - SYNC vers SQL Database (validations) via pyodbc\n",
    "    - Envoie emails pour les nouvelles validations\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODE IMPORT: Excel → Lakehouse + SQL + Emails\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Lire le fichier Excel\n",
    "    print(\"\\n1. Lecture Excel...\")\n",
    "    if not file_exists(EXCEL_PATH):\n",
    "        print(f\"   ✗ Fichier non trouvé: {EXCEL_PATH}\")\n",
    "        print(\"   Exécutez d'abord le mode 'export'.\")\n",
    "        return None\n",
    "    \n",
    "    df_excel = pd.read_excel(EXCEL_PATH)\n",
    "    print(f\"   {len(df_excel)} lignes lues\")\n",
    "    \n",
    "    # 2. Filtrer les lignes validées (is_validated != NULL)\n",
    "    df_validated = df_excel[df_excel[\"is_validated\"].notna()].copy()\n",
    "    print(f\"   {len(df_validated)} lignes validées (is_validated != NULL)\")\n",
    "    \n",
    "    if len(df_validated) == 0:\n",
    "        print(\"   Aucune validation à importer.\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Identifier les NOUVELLES validations (email_sent_at IS NULL)\n",
    "    df_new_validations = df_validated[df_validated[\"email_sent_at\"].isna()].copy()\n",
    "    print(f\"   {len(df_new_validations)} NOUVELLES validations (email non envoyé)\")\n",
    "    \n",
    "    # 4. Convertir en Spark DataFrame\n",
    "    print(\"\\n2. Conversion Spark...\")\n",
    "    \n",
    "    # Préparer les données\n",
    "    df_validated[\"is_validated\"] = df_validated[\"is_validated\"].astype(int)\n",
    "    df_validated[\"validation_date\"] = datetime.now()\n",
    "    df_validated[\"created_at\"] = datetime.now()\n",
    "    df_validated[\"updated_at\"] = datetime.now()\n",
    "    \n",
    "    # S'assurer que les colonnes existent\n",
    "    for col_name in [\"ingestion_week\", \"is_duplicate\", \"is_suspected_duplicate\", \"duplicate_score\"]:\n",
    "        if col_name not in df_validated.columns:\n",
    "            df_validated[col_name] = None\n",
    "    \n",
    "    # Renommer opportunity_id si nécessaire\n",
    "    if \"id\" in df_validated.columns and \"opportunity_id\" not in df_validated.columns:\n",
    "        df_validated = df_validated.rename(columns={\"id\": \"opportunity_id\"})\n",
    "    \n",
    "    sdf_validations = spark.createDataFrame(df_validated)\n",
    "    print(f\"   {sdf_validations.count()} lignes converties\")\n",
    "    \n",
    "    # 5. MERGE dans Lakehouse (landing_validations)\n",
    "    print(\"\\n3. MERGE dans Lakehouse...\")\n",
    "    \n",
    "    if not spark.catalog.tableExists(TABLE_VALIDATIONS):\n",
    "        print(f\"   Création de la table {TABLE_VALIDATIONS}...\")\n",
    "        sdf_validations.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_VALIDATIONS)\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, TABLE_VALIDATIONS)\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            sdf_validations.alias(\"source\"),\n",
    "            \"target.opportunity_id = source.opportunity_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\n",
    "                \"is_validated\": \"source.is_validated\",\n",
    "                \"validation_comment\": \"source.validation_comment\",\n",
    "                \"validated_by\": \"source.validated_by\",\n",
    "                \"validation_date\": \"source.validation_date\",\n",
    "                \"email_sent_at\": \"source.email_sent_at\",\n",
    "                \"ingestion_week\": \"source.ingestion_week\",\n",
    "                \"is_duplicate\": \"source.is_duplicate\",\n",
    "                \"is_suspected_duplicate\": \"source.is_suspected_duplicate\",\n",
    "                \"duplicate_score\": \"source.duplicate_score\",\n",
    "                \"updated_at\": \"source.updated_at\"\n",
    "            }\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    total_lakehouse = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_VALIDATIONS}\").collect()[0][0]\n",
    "    print(f\"   ✓ Table {TABLE_VALIDATIONS}: {total_lakehouse} lignes\")\n",
    "    \n",
    "    # 6. Envoyer les emails pour les nouvelles validations\n",
    "    print(\"\\n4. Envoi des emails...\")\n",
    "    emails_sent = 0\n",
    "    emails_to_update = []\n",
    "    \n",
    "    for idx, row in df_new_validations.iterrows():\n",
    "        saler_email = row.get(\"saler_email\")\n",
    "        if pd.isna(saler_email) or not saler_email:\n",
    "            print(f\"  - {row.get('venue_name', 'N/A')}: Pas d'email saler\")\n",
    "            continue\n",
    "        \n",
    "        # Formater et envoyer l'email\n",
    "        status = \"OK\" if row[\"is_validated\"] == 1 else \"KO\"\n",
    "        subject = f\"[Weak Signals] Opportunité {status}: {row.get('venue_name', 'N/A')}\"\n",
    "        body = format_validation_email(row)\n",
    "        \n",
    "        if send_email(saler_email, subject, body):\n",
    "            emails_sent += 1\n",
    "            emails_to_update.append(row[\"opportunity_id\"])\n",
    "    \n",
    "    print(f\"   ✓ {emails_sent} emails envoyés\")\n",
    "    \n",
    "    # 7. Mettre à jour email_sent_at dans Excel\n",
    "    if emails_to_update:\n",
    "        print(\"\\n5. Mise à jour email_sent_at dans Excel...\")\n",
    "        now = datetime.now()\n",
    "        \n",
    "        for opp_id in emails_to_update:\n",
    "            df_excel.loc[df_excel[\"opportunity_id\"] == opp_id, \"email_sent_at\"] = now\n",
    "        \n",
    "        df_excel.to_excel(EXCEL_PATH, index=False)\n",
    "        print(f\"   ✓ {len(emails_to_update)} lignes mises à jour\")\n",
    "        \n",
    "        # Mettre à jour aussi dans Lakehouse\n",
    "        print(\"\\n6. Mise à jour email_sent_at dans Lakehouse...\")\n",
    "        for opp_id in emails_to_update:\n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE {TABLE_VALIDATIONS}\n",
    "                SET email_sent_at = current_timestamp(), updated_at = current_timestamp()\n",
    "                WHERE opportunity_id = '{opp_id}'\n",
    "            \"\"\")\n",
    "        print(f\"   ✓ Lakehouse mis à jour\")\n",
    "    \n",
    "    # Résumé\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RÉSUMÉ IMPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Validations importées: {len(df_validated)}\")\n",
    "    print(f\"Nouvelles validations: {len(df_new_validations)}\")\n",
    "    print(f\"Emails envoyés: {emails_sent}\")\n",
    "    \n",
    "    return df_validated\n",
    "\n",
    "\n",
    "# Exécuter si mode import ou full_sync\n",
    "if MODE in [\"import\", \"full_sync\"]:\n",
    "    df_imported = run_import()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de la table landing_validations\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if spark.catalog.tableExists(TABLE_VALIDATIONS):\n",
    "    print(f\"\\nTable {TABLE_VALIDATIONS}:\")\n",
    "    \n",
    "    # Compter par statut et semaine\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            ingestion_week,\n",
    "            CASE \n",
    "                WHEN is_validated = 1 THEN 'OK'\n",
    "                WHEN is_validated = 0 THEN 'KO'\n",
    "                ELSE 'PENDING'\n",
    "            END as statut,\n",
    "            COUNT(*) as nb,\n",
    "            SUM(CASE WHEN email_sent_at IS NOT NULL THEN 1 ELSE 0 END) as emails_envoyes\n",
    "        FROM {TABLE_VALIDATIONS}\n",
    "        GROUP BY ingestion_week, is_validated\n",
    "        ORDER BY ingestion_week DESC, statut\n",
    "    \"\"\").show(20)\n",
    "    \n",
    "    # Validations sans email\n",
    "    pending_emails = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) FROM {TABLE_VALIDATIONS}\n",
    "        WHERE is_validated IS NOT NULL AND email_sent_at IS NULL\n",
    "    \"\"\").collect()[0][0]\n",
    "    \n",
    "    if pending_emails > 0:\n",
    "        print(f\"⚠ {pending_emails} validations sans email envoyé\")\n",
    "    \n",
    "    # Stats déduplication\n",
    "    print(\"\\nStatistiques déduplication:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            ingestion_week,\n",
    "            SUM(CASE WHEN is_suspected_duplicate = 1 THEN 1 ELSE 0 END) as zone_grise,\n",
    "            COUNT(*) as total\n",
    "        FROM {TABLE_VALIDATIONS}\n",
    "        GROUP BY ingestion_week\n",
    "        ORDER BY ingestion_week DESC\n",
    "    \"\"\").show(10)\n",
    "else:\n",
    "    print(f\"Table {TABLE_VALIDATIONS} n'existe pas encore.\")\n",
    "\n",
    "# Stats fichier Excel\n",
    "if file_exists(EXCEL_PATH):\n",
    "    df_stats = pd.read_excel(EXCEL_PATH)\n",
    "    print(f\"\\nFichier Excel ({EXCEL_PATH}):\")\n",
    "    print(f\"  Total lignes: {len(df_stats)}\")\n",
    "    print(f\"  Validées (OK): {len(df_stats[df_stats['is_validated'] == 1])}\")\n",
    "    print(f\"  Rejetées (KO): {len(df_stats[df_stats['is_validated'] == 0])}\")\n",
    "    print(f\"  En attente: {len(df_stats[df_stats['is_validated'].isna()])}\")\n",
    "    \n",
    "    # Stats par semaine\n",
    "    if \"ingestion_week\" in df_stats.columns:\n",
    "        print(f\"\\n  Par semaine:\")\n",
    "        print(df_stats.groupby(\"ingestion_week\").size().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Historique Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(TABLE_VALIDATIONS):\n",
    "    print(f\"Historique Delta de '{TABLE_VALIDATIONS}':\")\n",
    "    spark.sql(f\"DESCRIBE HISTORY {TABLE_VALIDATIONS}\").select(\n",
    "        \"version\", \"timestamp\", \"operation\", \"operationMetrics\"\n",
    "    ).show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Guide d'utilisation\n",
    "\n",
    "### Configuration\n",
    "\n",
    "| Variable | Description | Exemple |\n",
    "|----------|-------------|---------|\n",
    "| `MODE` | Mode d'exécution | `\"export\"`, `\"import\"`, `\"full_sync\"` |\n",
    "| `INGESTION_WEEK` | Semaine à exporter | `\"2024-W49\"` ou `None` (auto) |\n",
    "\n",
    "### Flux hebdomadaire\n",
    "\n",
    "```\n",
    "Lundi matin:\n",
    "1. Exécuter deduplicate_weekly.ipynb (déduplication)\n",
    "2. Exécuter sync_validations_excel.ipynb MODE=\"export\"\n",
    "3. Télécharger le fichier Excel\n",
    "\n",
    "Pendant la semaine:\n",
    "4. Valider les opportunités dans Excel\n",
    "   - is_validated: 1 (OK) ou 0 (KO)\n",
    "   - validation_comment: commentaire optionnel\n",
    "   - validated_by: votre email\n",
    "\n",
    "Vendredi:\n",
    "5. Uploader le fichier Excel\n",
    "6. Exécuter sync_validations_excel.ipynb MODE=\"import\"\n",
    "7. Les emails sont envoyés automatiquement aux salers\n",
    "```\n",
    "\n",
    "### Colonnes Excel\n",
    "\n",
    "| Colonne | Description | Rempli par |\n",
    "|---------|-------------|------------|\n",
    "| `opportunity_id` | ID unique | Auto |\n",
    "| `ingestion_week` | Semaine d'ingestion | Auto |\n",
    "| `article_title` | Titre de l'article | Auto |\n",
    "| `venue_name` | Nom du lieu | Auto |\n",
    "| `is_duplicate` | Doublon confirmé | Auto (0/1) |\n",
    "| `is_suspected_duplicate` | Zone grise | Auto (0/1) |\n",
    "| `duplicate_score` | Score similarité | Auto (0.0-1.0) |\n",
    "| `is_validated` | **Validation** | **User** (1=OK, 0=KO) |\n",
    "| `validation_comment` | Commentaire | User |\n",
    "| `validated_by` | Email validateur | User |\n",
    "| `email_sent_at` | Email envoyé | Auto |\n",
    "\n",
    "### Configuration SMTP\n",
    "Remplir les variables en haut du notebook:\n",
    "- `SMTP_USER`: votre email @l-acoustics.com\n",
    "- `SMTP_PASSWORD`: votre App Password (créé sur https://mysignins.microsoft.com/security-info)\n",
    "\n",
    "### Déduplication\n",
    "\n",
    "Les articles sont filtrés avant export:\n",
    "- **Doublons confirmés** (score ≥ 0.90): **Exclus** automatiquement\n",
    "- **Zone grise** (0.85 ≤ score < 0.90): **Inclus** avec warning ⚠️\n",
    "- **Uniques** (score < 0.85): **Inclus** normalement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
