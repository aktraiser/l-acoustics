{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sync Validations - EXPORT\n\nExport des opportunités depuis le Lakehouse vers Excel pour validation manuelle.\n\n## Fréquence\n- **CRON**: Hebdomadaire (lundi matin, après `deduplicate_weekly.ipynb`)\n\n## Flux\n```\nlanding_feedly_opportunities + landing_salers → Excel (nouveau fichier chaque semaine)\n```\n\n## Filtres appliqués\n- `audit_opportunity = 1`\n- `is_duplicate = false` (exclut les doublons confirmés)\n- `ingestion_week = semaine courante`\n\n## Fichier Excel\n- **Emplacement**: `/lakehouse/default/Files/weak_signals_validation.xlsx`\n- **Accès users**: OneLake File Explorer ou téléchargement depuis Fabric\n- **Note**: Le fichier est SUPPRIMÉ et recréé chaque semaine (pas de cumul)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\n# Semaine d'ingestion à exporter (format: \"2024-W49\")\n# \"auto\" = semaine courante automatique (défaut)\nINGESTION_WEEK = \"auto\"\n\n# Chemin du fichier Excel dans le Lakehouse\nEXCEL_PATH = \"/lakehouse/default/Files/weak_signals_validation.xlsx\"\n\n# Tables Lakehouse\nTABLE_OPPORTUNITIES = \"landing_feedly_opportunities\"\nTABLE_SALERS = \"landing_salers\"\nTABLE_SALERS_BD = \"landing_salers_bd\"\n\n# Calculer la semaine courante si \"auto\"\nfrom datetime import datetime\nif INGESTION_WEEK == \"auto\":\n    INGESTION_WEEK = datetime.now().strftime(\"%Y-W%V\")\n\nprint(f\"Semaine d'ingestion: {INGESTION_WEEK}\")\nprint(f\"Excel: {EXCEL_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports et Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, lower\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark session ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def file_exists(path):\n    \"\"\"Vérifie si un fichier existe dans le Lakehouse.\"\"\"\n    try:\n        mssparkutils.fs.head(path, 1)\n        return True\n    except:\n        return os.path.exists(path)\n\n\ndef delete_file(path):\n    \"\"\"Supprime un fichier dans le Lakehouse.\"\"\"\n    try:\n        mssparkutils.fs.rm(path, recurse=False)\n        return True\n    except:\n        try:\n            os.remove(path)\n            return True\n        except:\n            return False\n\n\ndef get_opportunities_with_salers(ingestion_week):\n    \"\"\"\n    Récupère les opportunités (audit_opportunity=1) avec les salers assignés.\n    \n    Filtres:\n    - audit_opportunity = 1\n    - is_duplicate = false OU NULL (exclut les doublons confirmés)\n    - ingestion_week = semaine spécifiée\n    \n    Jointure sur country pour RAE/RSM.\n    \"\"\"\n    query = f\"\"\"\n    SELECT \n        o.id,\n        o.ingestion_week,\n        o.article_title,\n        o.article_url,\n        o.venue_name,\n        o.city,\n        o.country,\n        o.vertical,\n        o.evaluation_score,\n        o.audit_opportunity_reason,\n        -- Déduplication\n        o.is_duplicate,\n        o.is_suspected_duplicate,\n        o.duplicate_score,\n        -- Saler\n        s.who AS saler_name,\n        s.email AS saler_email\n    FROM {TABLE_OPPORTUNITIES} o\n    LEFT JOIN {TABLE_SALERS} s \n        ON TRIM(LOWER(o.country)) = TRIM(LOWER(s.country))\n    WHERE o.audit_opportunity = 1\n      AND (o.is_duplicate = false OR o.is_duplicate IS NULL)\n      AND o.ingestion_week = '{ingestion_week}'\n    ORDER BY o.evaluation_score DESC\n    \"\"\"\n    return spark.sql(query)\n\n\nprint(\"Fonctions utilitaires chargées\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export: Lakehouse → Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_export():\n    \"\"\"\n    Export les opportunités de la semaine vers Excel.\n    \n    IMPORTANT: Supprime l'Excel existant et crée un nouveau fichier\n    uniquement avec les opportunités de la semaine courante.\n    \n    Filtres appliqués:\n    - ingestion_week = INGESTION_WEEK\n    - is_duplicate = false (exclut les doublons confirmés)\n    - audit_opportunity = 1\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"EXPORT: Lakehouse → Excel\")\n    print(\"=\" * 60)\n    print(f\"Semaine d'ingestion: {INGESTION_WEEK}\")\n    \n    # 1. Supprimer l'Excel existant\n    print(\"\\n1. Suppression de l'Excel existant...\")\n    if file_exists(EXCEL_PATH):\n        if delete_file(EXCEL_PATH):\n            print(f\"   ✓ Fichier supprimé: {EXCEL_PATH}\")\n        else:\n            print(f\"   ⚠️ Impossible de supprimer: {EXCEL_PATH}\")\n    else:\n        print(\"   Aucun fichier existant.\")\n    \n    # 2. Récupérer les opportunités avec salers (filtrées sur la semaine)\n    print(\"\\n2. Récupération des opportunités...\")\n    df_opportunities = get_opportunities_with_salers(ingestion_week=INGESTION_WEEK).toPandas()\n    print(f\"   {len(df_opportunities)} opportunités trouvées pour {INGESTION_WEEK}\")\n    \n    if len(df_opportunities) == 0:\n        print(\"   Aucune opportunité à exporter pour cette semaine.\")\n        return None\n    \n    # Stats déduplication\n    n_suspected = len(df_opportunities[df_opportunities[\"is_suspected_duplicate\"] == True])\n    if n_suspected > 0:\n        print(f\"   ⚠️ {n_suspected} articles en zone grise (doublons potentiels)\")\n    \n    # 3. Préparer le DataFrame avec colonnes validation vides\n    print(\"\\n3. Préparation des données...\")\n    df_export = df_opportunities.rename(columns={\"id\": \"opportunity_id\"})\n    df_export[\"is_validated\"] = None\n    df_export[\"validation_comment\"] = None\n    df_export[\"validated_by\"] = None\n    df_export[\"email_sent_at\"] = None\n    \n    # 4. Écrire le fichier Excel\n    print(\"\\n4. Écriture Excel...\")\n    \n    # Ordre des colonnes\n    columns_order = [\n        \"opportunity_id\",\n        \"ingestion_week\",\n        \"article_title\",\n        \"article_url\",\n        \"venue_name\",\n        \"city\",\n        \"country\",\n        \"vertical\",\n        \"evaluation_score\",\n        \"audit_opportunity_reason\",\n        \"is_duplicate\",\n        \"is_suspected_duplicate\",\n        \"duplicate_score\",\n        \"saler_name\",\n        \"saler_email\",\n        \"is_validated\",\n        \"validation_comment\",\n        \"validated_by\",\n        \"email_sent_at\"\n    ]\n    \n    # S'assurer que toutes les colonnes existent\n    for c in columns_order:\n        if c not in df_export.columns:\n            df_export[c] = None\n    \n    df_export = df_export[columns_order]\n    df_export.to_excel(EXCEL_PATH, index=False)\n    \n    print(f\"   ✓ Fichier créé: {EXCEL_PATH}\")\n    print(f\"   ✓ {len(df_export)} lignes exportées\")\n    \n    return df_export\n\n\n# Exécuter l'export\ndf_exported = run_export()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistiques Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques du fichier Excel\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES EXCEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if file_exists(EXCEL_PATH):\n",
    "    df_stats = pd.read_excel(EXCEL_PATH)\n",
    "    print(f\"\\nFichier: {EXCEL_PATH}\")\n",
    "    print(f\"  Total lignes: {len(df_stats)}\")\n",
    "    print(f\"  Validées (OK): {len(df_stats[df_stats['is_validated'] == 1])}\")\n",
    "    print(f\"  Rejetées (KO): {len(df_stats[df_stats['is_validated'] == 0])}\")\n",
    "    print(f\"  En attente: {len(df_stats[df_stats['is_validated'].isna()])}\")\n",
    "    \n",
    "    # Stats par semaine\n",
    "    if \"ingestion_week\" in df_stats.columns:\n",
    "        print(f\"\\n  Par semaine:\")\n",
    "        week_stats = df_stats.groupby(\"ingestion_week\").agg({\n",
    "            \"opportunity_id\": \"count\",\n",
    "            \"is_validated\": lambda x: x.notna().sum()\n",
    "        }).rename(columns={\"opportunity_id\": \"total\", \"is_validated\": \"validées\"})\n",
    "        print(week_stats.to_string())\n",
    "    \n",
    "    # Stats zone grise\n",
    "    if \"is_suspected_duplicate\" in df_stats.columns:\n",
    "        n_grey = len(df_stats[df_stats[\"is_suspected_duplicate\"] == True])\n",
    "        if n_grey > 0:\n",
    "            print(f\"\\n   {n_grey} articles en zone grise\")\n",
    "else:\n",
    "    print(f\"Fichier non trouvé: {EXCEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aperçu des données exportées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des premières lignes\n",
    "if df_exported is not None:\n",
    "    print(\"Aperçu des données exportées:\")\n",
    "    display(df_exported[[\"opportunity_id\", \"ingestion_week\", \"venue_name\", \"city\", \"country\", \"evaluation_score\", \"is_validated\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Guide d'utilisation\n\n### Configuration\n\n| Variable | Description | Exemple |\n|----------|-------------|--------|\n| `INGESTION_WEEK` | Semaine à exporter | `\"2024-W49\"` ou `\"auto\"` (défaut) |\n\n### Comportement\n\n- **L'Excel existant est SUPPRIMÉ** à chaque exécution\n- Un nouveau fichier est créé avec uniquement la semaine spécifiée\n- Pas de cumul des semaines précédentes\n\n### Après l'export\n\n1. Télécharger le fichier Excel depuis Fabric ou OneLake File Explorer\n2. Valider les opportunités:\n   - `is_validated`: 1 (OK) ou 0 (KO)\n   - `validation_comment`: commentaire optionnel\n   - `validated_by`: votre email\n3. Uploader le fichier Excel modifié\n4. Exécuter `sync_validations_import.ipynb`\n\n### Colonnes Excel\n\n| Colonne | Description | Rempli par |\n|---------|-------------|------------|\n| `opportunity_id` | ID unique | Auto |\n| `ingestion_week` | Semaine d'ingestion | Auto |\n| `article_title` | Titre de l'article | Auto |\n| `venue_name` | Nom du lieu | Auto |\n| `is_suspected_duplicate` | Zone grise | Auto (True/False) |\n| `is_validated` | **Validation** | **User** (1=OK, 0=KO) |\n| `validation_comment` | Commentaire | User |\n| `validated_by` | Email validateur | User |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}