{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©duplication Hebdomadaire\n",
    "\n",
    "D√©tecte les doublons s√©mantiques parmi les articles de la semaine courante en les comparant √† l'historique complet.\n",
    "\n",
    "## Principe\n",
    "\n",
    "Utilise `ai.similarity()` de Fabric AI Functions pour comparer le texte :\n",
    "- **Texte compar√©** : `title + venue_name + city + country`\n",
    "- **Seuils** :\n",
    "  - `>= 0.90` : Doublon confirm√© (`is_duplicate = true`)\n",
    "  - `0.85 - 0.90` : Zone grise (`is_suspected_duplicate = true`)\n",
    "  - `< 0.85` : Unique\n",
    "\n",
    "## Ordonnancement\n",
    "\n",
    "```\n",
    "# Lundi 6h00 - Ingestion des articles\n",
    "landing_feedly_opportunities.ipynb (mode incremental)\n",
    "\n",
    "# Lundi 7h00 - D√©duplication (CE NOTEBOOK)\n",
    "deduplicate_weekly.ipynb\n",
    "\n",
    "# Lundi 8h00 - Export Excel\n",
    "sync_validations_excel.ipynb (mode export)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Table source\nTABLE_LANDING = \"landing_feedly_opportunities\"\n\n# Seuils de similarit√©\nTHRESHOLD_DUPLICATE = 0.90      # >= 0.90 = doublon confirm√©\nTHRESHOLD_SUSPECTED = 0.85      # >= 0.85 et < 0.90 = zone grise\n\n# Mode debug (affiche plus de d√©tails)\nDEBUG_MODE = False\n\n# Mode FORCE_REPROCESS: retraiter tous les articles de la semaine m√™me s'ils ont d√©j√† √©t√© trait√©s\nFORCE_REPROCESS = True  # Mettre √† False en production\n\nprint(f\"Table: {TABLE_LANDING}\")\nprint(f\"Seuil doublon confirm√©: >= {THRESHOLD_DUPLICATE}\")\nprint(f\"Seuil zone grise: >= {THRESHOLD_SUSPECTED} et < {THRESHOLD_DUPLICATE}\")\nprint(f\"Force reprocess: {FORCE_REPROCESS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports et Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import (\n    col, lit, concat_ws, coalesce, when,\n    max as spark_max, first, row_number\n)\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import DoubleType, BooleanType, StringType\nfrom datetime import datetime\nfrom delta.tables import DeltaTable\n\nimport synapse.ml.spark.aifunc as aifunc\n\nspark = SparkSession.builder.getOrCreate()\n\n# Semaine courante au format ISO 8601 (ex: \"2024-W49\")\ncurrent_week = datetime.now().strftime(\"%Y-W%V\")\nprint(f\"Spark session ready\")\nprint(f\"AI Functions loaded\")\nprint(f\"Semaine courante: {current_week}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Articles de la semaine courante √† traiter\nif FORCE_REPROCESS:\n    # Mode force: retraiter TOUS les articles de la semaine\n    new_articles = spark.sql(f\"\"\"\n        SELECT \n            id,\n            article_title,\n            venue_name,\n            city,\n            country,\n            ingestion_week\n        FROM {TABLE_LANDING}\n        WHERE ingestion_week = '{current_week}'\n    \"\"\")\n    print(f\"‚ö†Ô∏è Mode FORCE_REPROCESS activ√© - tous les articles de la semaine seront retrait√©s\")\nelse:\n    # Mode normal: seulement les articles non encore trait√©s\n    new_articles = spark.sql(f\"\"\"\n        SELECT \n            id,\n            article_title,\n            venue_name,\n            city,\n            country,\n            ingestion_week\n        FROM {TABLE_LANDING}\n        WHERE ingestion_week = '{current_week}'\n          AND is_duplicate IS NULL\n    \"\"\")\n\n# Historique (semaines pr√©c√©dentes uniquement en mode FORCE_REPROCESS)\nif FORCE_REPROCESS:\n    # En mode force, l'historique = seulement les semaines pr√©c√©dentes\n    historical = spark.sql(f\"\"\"\n        SELECT \n            id,\n            article_title,\n            venue_name,\n            city,\n            country,\n            ingestion_week\n        FROM {TABLE_LANDING}\n        WHERE ingestion_week != '{current_week}'\n    \"\"\")\nelse:\n    # Mode normal: semaines pr√©c√©dentes + articles d√©j√† trait√©s de cette semaine\n    historical = spark.sql(f\"\"\"\n        SELECT \n            id,\n            article_title,\n            venue_name,\n            city,\n            country,\n            ingestion_week\n        FROM {TABLE_LANDING}\n        WHERE ingestion_week != '{current_week}'\n           OR (ingestion_week = '{current_week}' AND is_duplicate IS NOT NULL)\n    \"\"\")\n\nnew_count = new_articles.count()\nhist_count = historical.count()\n\nprint(f\"üì• Articles √† traiter (semaine {current_week}): {new_count}\")\nprint(f\"üìö Articles historiques (r√©f√©rence): {hist_count}\")\n\nif new_count == 0:\n    print(\"\\n‚úÖ Aucun nouvel article √† traiter cette semaine.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration du texte √† comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_count > 0:\n",
    "    # Cr√©er le texte √† comparer pour les nouveaux articles\n",
    "    # Format: \"title | venue_name | city | country\"\n",
    "    new_articles_with_text = new_articles.withColumn(\n",
    "        \"text_to_compare\",\n",
    "        concat_ws(\n",
    "            \" | \",\n",
    "            coalesce(col(\"article_title\"), lit(\"\")),\n",
    "            coalesce(col(\"venue_name\"), lit(\"\")),\n",
    "            coalesce(col(\"city\"), lit(\"\")),\n",
    "            coalesce(col(\"country\"), lit(\"\"))\n",
    "        )\n",
    "    ).select(\"id\", \"text_to_compare\", \"article_title\")\n",
    "    \n",
    "    # Cr√©er le texte √† comparer pour l'historique\n",
    "    historical_with_text = historical.withColumn(\n",
    "        \"text_to_compare\",\n",
    "        concat_ws(\n",
    "            \" | \",\n",
    "            coalesce(col(\"article_title\"), lit(\"\")),\n",
    "            coalesce(col(\"venue_name\"), lit(\"\")),\n",
    "            coalesce(col(\"city\"), lit(\"\")),\n",
    "            coalesce(col(\"country\"), lit(\"\"))\n",
    "        )\n",
    "    ).select(\n",
    "        col(\"id\").alias(\"hist_id\"),\n",
    "        col(\"text_to_compare\").alias(\"hist_text\"),\n",
    "        col(\"article_title\").alias(\"hist_title\")\n",
    "    )\n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(\"\\nüîç Exemples de textes √† comparer (nouveaux):\")\n",
    "        new_articles_with_text.show(5, truncate=60)\n",
    "    \n",
    "    print(f\"‚úÖ Textes pr√©par√©s pour la comparaison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calcul de similarit√© avec Fabric AI Functions\n",
    "\n",
    "Utilise `ai.similarity()` pour calculer la similarit√© s√©mantique :\n",
    "1. **Intra-semaine** : Compare les nouveaux articles entre eux (m√™me semaine)\n",
    "2. **Historique** : Compare les nouveaux articles avec l'historique (semaines pr√©c√©dentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if new_count > 0:\n    # =========================================================================\n    # ETAPE 1: Comparaison INTRA-SEMAINE (nouveaux articles entre eux)\n    # =========================================================================\n    # Important: 2 articles sur le m√™me √©v√©nement peuvent arriver la m√™me semaine\n    \n    print(\"üìä √âtape 1: Comparaison intra-semaine (nouveaux articles entre eux)...\")\n    print(f\"   Nombre d'articles √† comparer entre eux: {new_count}\")\n    \n    # Ajouter un index num√©rique pour la comparaison (√©vite les probl√®mes avec les IDs string)\n    from pyspark.sql.functions import monotonically_increasing_id\n    \n    new_with_idx = new_articles_with_text.withColumn(\"idx\", monotonically_increasing_id())\n    \n    # Debug: afficher les articles\n    print(\"\\nüîç Debug - Articles √† traiter:\")\n    new_with_idx.select(\"idx\", \"id\", \"article_title\", \"text_to_compare\").show(5, truncate=50)\n    \n    # Self-join pour comparer chaque article avec tous les autres\n    new_left = new_with_idx.select(\n        col(\"idx\").alias(\"left_idx\"),\n        col(\"id\").alias(\"left_id\"),\n        col(\"text_to_compare\").alias(\"left_text\"),\n        col(\"article_title\").alias(\"left_title\")\n    )\n    \n    new_right = new_with_idx.select(\n        col(\"idx\").alias(\"right_idx\"),\n        col(\"id\").alias(\"right_id\"),\n        col(\"text_to_compare\").alias(\"right_text\"),\n        col(\"article_title\").alias(\"right_title\")\n    )\n    \n    # Cross join et filtre pour √©viter comparaison avec soi-m√™me et doublons (A vs B, pas B vs A)\n    intra_week_df = new_left.crossJoin(new_right).filter(\n        col(\"left_idx\") < col(\"right_idx\")  # Comparaison num√©rique fiable\n    )\n    \n    intra_comparisons = intra_week_df.count()\n    expected_comparisons = (new_count * (new_count - 1)) // 2\n    print(f\"   Comparaisons intra-semaine: {intra_comparisons} (attendu: {expected_comparisons})\")\n    \n    if intra_comparisons > 0:\n        # Debug: afficher quelques paires √† comparer\n        print(\"\\nüîç Debug - Exemples de paires √† comparer:\")\n        intra_week_df.select(\"left_id\", \"left_title\", \"right_id\", \"right_title\").show(5, truncate=40)\n        \n        # Calculer la similarit√© intra-semaine avec df.ai.similarity()\n        print(\"\\n   Calcul des similarit√©s intra-semaine...\")\n        intra_similarity = intra_week_df.select(\n            col(\"left_id\").alias(\"new_id\"),\n            col(\"left_text\").alias(\"new_text\"),\n            col(\"left_title\").alias(\"new_title\"),\n            col(\"right_id\").alias(\"hist_id\"),\n            col(\"right_text\").alias(\"hist_text\"),\n            col(\"right_title\").alias(\"hist_title\")\n        ).ai.similarity(\n            input_col=\"new_text\",\n            other_col=\"hist_text\",\n            output_col=\"similarity_score\"\n        )\n        \n        # Aussi comparer dans l'autre sens (B vs A) pour avoir le meilleur match pour chaque article\n        intra_similarity_reverse = intra_week_df.select(\n            col(\"right_id\").alias(\"new_id\"),\n            col(\"right_text\").alias(\"new_text\"),\n            col(\"right_title\").alias(\"new_title\"),\n            col(\"left_id\").alias(\"hist_id\"),\n            col(\"left_text\").alias(\"hist_text\"),\n            col(\"left_title\").alias(\"hist_title\")\n        ).ai.similarity(\n            input_col=\"new_text\",\n            other_col=\"hist_text\",\n            output_col=\"similarity_score\"\n        )\n        \n        intra_similarity_all = intra_similarity.union(intra_similarity_reverse)\n        intra_count = intra_similarity_all.count()\n        print(f\"   ‚úÖ Similarit√©s intra-semaine calcul√©es: {intra_count} comparaisons\")\n    else:\n        intra_similarity_all = None\n        print(\"   ‚ö†Ô∏è Pas assez d'articles pour la comparaison intra-semaine\")\n    \n    # =========================================================================\n    # ETAPE 2: Comparaison avec l'HISTORIQUE (semaines pr√©c√©dentes)\n    # =========================================================================\n    \n    if hist_count > 0:\n        print(f\"\\nüìä √âtape 2: Comparaison avec l'historique ({hist_count} articles)...\")\n        \n        cross_df = new_articles_with_text.crossJoin(historical_with_text)\n        hist_comparisons = new_count * hist_count\n        print(f\"   Comparaisons historique: {hist_comparisons}\")\n        \n        # Calculer la similarit√© avec l'historique\n        hist_similarity = cross_df.select(\n            col(\"id\").alias(\"new_id\"),\n            col(\"text_to_compare\").alias(\"new_text\"),\n            col(\"article_title\").alias(\"new_title\"),\n            col(\"hist_id\"),\n            col(\"hist_text\"),\n            col(\"hist_title\")\n        ).ai.similarity(\n            input_col=\"new_text\",\n            other_col=\"hist_text\",\n            output_col=\"similarity_score\"\n        )\n        print(f\"   ‚úÖ Similarit√©s historique calcul√©es\")\n    else:\n        hist_similarity = None\n        print(\"\\nüìö Pas d'historique disponible - seule la comparaison intra-semaine sera utilis√©e\")\n    \n    # =========================================================================\n    # ETAPE 3: Combiner les r√©sultats\n    # =========================================================================\n    \n    print(\"\\nüìä √âtape 3: Fusion des r√©sultats...\")\n    \n    # Combiner intra-semaine et historique\n    if intra_similarity_all is not None and hist_similarity is not None:\n        all_similarities = intra_similarity_all.union(hist_similarity)\n        print(f\"   Sources: intra-semaine + historique\")\n    elif intra_similarity_all is not None:\n        all_similarities = intra_similarity_all\n        print(f\"   Sources: intra-semaine uniquement\")\n    elif hist_similarity is not None:\n        all_similarities = hist_similarity\n        print(f\"   Sources: historique uniquement\")\n    else:\n        all_similarities = None\n        print(f\"   ‚ö†Ô∏è Aucune source de comparaison\")\n    \n    # =========================================================================\n    # ETAPE 4: Classification de TOUS les articles\n    # =========================================================================\n    \n    print(\"\\nüìä √âtape 4: Classification des articles...\")\n    \n    # Cr√©er un DataFrame de base avec tous les nouveaux articles\n    all_new_articles = new_articles_with_text.select(\n        col(\"id\").alias(\"new_id\"),\n        col(\"article_title\").alias(\"new_title\")\n    )\n    \n    if all_similarities is not None:\n        total_similarities = all_similarities.count()\n        print(f\"   Total des similarit√©s calcul√©es: {total_similarities}\")\n        \n        # Debug: afficher les meilleures similarit√©s\n        print(\"\\nüîç Debug - Top 10 des similarit√©s les plus √©lev√©es:\")\n        all_similarities.select(\n            \"new_id\", \"new_title\", \"hist_title\", \"similarity_score\"\n        ).orderBy(col(\"similarity_score\").desc()).show(10, truncate=50)\n        \n        # Fen√™tre par new_id, ordonn√©e par score d√©croissant\n        window_spec = Window.partitionBy(\"new_id\").orderBy(col(\"similarity_score\").desc())\n        \n        # Ajouter le rang et filtrer pour garder seulement le meilleur match\n        best_matches = all_similarities.withColumn(\n            \"rank\", row_number().over(window_spec)\n        ).filter(col(\"rank\") == 1).drop(\"rank\").select(\n            col(\"new_id\"),\n            col(\"similarity_score\").alias(\"max_similarity\"),\n            col(\"hist_id\").alias(\"best_match_id\"),\n            col(\"hist_title\").alias(\"best_match_title\")\n        )\n        \n        # LEFT JOIN pour inclure TOUS les articles, m√™me ceux sans match\n        classified = all_new_articles.join(\n            best_matches,\n            on=\"new_id\",\n            how=\"left\"\n        ).withColumn(\n            # Si pas de match trouv√©, mettre 0.0\n            \"max_similarity\", coalesce(col(\"max_similarity\"), lit(0.0))\n        ).withColumn(\n            \"is_duplicate\",\n            when(col(\"max_similarity\") >= THRESHOLD_DUPLICATE, True)\n            .otherwise(False)\n        ).withColumn(\n            \"is_suspected_duplicate\",\n            when(\n                (col(\"max_similarity\") >= THRESHOLD_SUSPECTED) & \n                (col(\"max_similarity\") < THRESHOLD_DUPLICATE),\n                True\n            ).otherwise(False)\n        ).withColumn(\n            \"duplicate_of\",\n            when(col(\"max_similarity\") >= THRESHOLD_SUSPECTED, col(\"best_match_id\"))\n            .otherwise(None)\n        )\n        \n    else:\n        # Pas de comparaison possible = tous uniques\n        print(\"   ‚ö†Ô∏è Aucune comparaison possible - tous les articles marqu√©s comme uniques\")\n        classified = all_new_articles.withColumn(\n            \"max_similarity\", lit(0.0)\n        ).withColumn(\n            \"is_duplicate\", lit(False)\n        ).withColumn(\n            \"is_suspected_duplicate\", lit(False)\n        ).withColumn(\n            \"duplicate_of\", lit(None).cast(StringType())\n        ).withColumn(\n            \"best_match_id\", lit(None).cast(StringType())\n        ).withColumn(\n            \"best_match_title\", lit(None).cast(StringType())\n        )\n    \n    # Stats\n    duplicates_count = classified.filter(col(\"is_duplicate\") == True).count()\n    suspected_count = classified.filter(col(\"is_suspected_duplicate\") == True).count()\n    unique_count = classified.filter(\n        (col(\"is_duplicate\") == False) & (col(\"is_suspected_duplicate\") == False)\n    ).count()\n    total_classified = classified.count()\n    \n    print(f\"\\n\" + \"=\"*60)\n    print(f\"üìà R√âSULTATS DE LA D√âDUPLICATION\")\n    print(f\"=\"*60)\n    print(f\"   Total articles classifi√©s: {total_classified} / {new_count}\")\n    print(f\"   ‚úÖ Doublons confirm√©s (>= {THRESHOLD_DUPLICATE}): {duplicates_count}\")\n    print(f\"   ‚ö†Ô∏è Zone grise ({THRESHOLD_SUSPECTED} - {THRESHOLD_DUPLICATE}): {suspected_count}\")\n    print(f\"   üÜï Uniques (< {THRESHOLD_SUSPECTED}): {unique_count}\")\n    print(f\"=\"*60)\n    \n    # Toujours afficher le d√©tail des articles avec doublons potentiels\n    if duplicates_count > 0 or suspected_count > 0:\n        print(\"\\nüîç D√©tail des articles avec doublons potentiels:\")\n        classified.filter(\n            col(\"max_similarity\") >= THRESHOLD_SUSPECTED\n        ).select(\n            \"new_id\", \"new_title\", \"max_similarity\", \n            \"is_duplicate\", \"is_suspected_duplicate\", \"best_match_title\"\n        ).orderBy(col(\"max_similarity\").desc()).show(20, truncate=40)\n    \n    # Debug: afficher tous les r√©sultats\n    if DEBUG_MODE:\n        print(\"\\nüîç Debug - Tous les articles classifi√©s:\")\n        classified.select(\n            \"new_id\", \"new_title\", \"max_similarity\", \n            \"is_duplicate\", \"is_suspected_duplicate\"\n        ).orderBy(col(\"max_similarity\").desc()).show(50, truncate=40)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mise √† jour de la table Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_count > 0:\n",
    "    # Pr√©parer le DataFrame pour le merge\n",
    "    updates_df = classified.select(\n",
    "        col(\"new_id\").alias(\"id\"),\n",
    "        col(\"is_duplicate\"),\n",
    "        col(\"is_suspected_duplicate\"),\n",
    "        col(\"duplicate_of\"),\n",
    "        col(\"max_similarity\").alias(\"duplicate_score\")\n",
    "    )\n",
    "    \n",
    "    # Merge dans la table Delta\n",
    "    delta_table = DeltaTable.forName(spark, TABLE_LANDING)\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        updates_df.alias(\"source\"),\n",
    "        \"target.id = source.id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\n",
    "            \"is_duplicate\": \"source.is_duplicate\",\n",
    "            \"is_suspected_duplicate\": \"source.is_suspected_duplicate\",\n",
    "            \"duplicate_of\": \"source.duplicate_of\",\n",
    "            \"duplicate_score\": \"source.duplicate_score\"\n",
    "        }\n",
    "    ).execute()\n",
    "    \n",
    "    print(f\"\\n Table '{TABLE_LANDING}' mise √† jour avec les r√©sultats de d√©duplication\")\n",
    "    print(f\"   - {duplicates_count + suspected_count + unique_count} articles trait√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistiques finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats de la semaine courante\n",
    "print(f\"\\n Statistiques de d√©duplication - Semaine {current_week}:\\n\")\n",
    "\n",
    "stats = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_articles,\n",
    "        SUM(CASE WHEN is_duplicate = true THEN 1 ELSE 0 END) as doublons_confirmes,\n",
    "        SUM(CASE WHEN is_suspected_duplicate = true THEN 1 ELSE 0 END) as zone_grise,\n",
    "        SUM(CASE WHEN is_duplicate = false AND is_suspected_duplicate = false THEN 1 ELSE 0 END) as uniques,\n",
    "        SUM(CASE WHEN is_duplicate IS NULL THEN 1 ELSE 0 END) as non_traites\n",
    "    FROM {TABLE_LANDING}\n",
    "    WHERE ingestion_week = '{current_week}'\n",
    "\"\"\")\n",
    "\n",
    "stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## R√©sum√©\n",
    "\n",
    "Ce notebook effectue la d√©duplication s√©mantique des articles Feedly :\n",
    "\n",
    "1. **Charge** les articles de la semaine courante (`ingestion_week`)\n",
    "2. **Compare intra-semaine** : chaque nouvel article avec les autres de la m√™me semaine\n",
    "3. **Compare avec l'historique** : chaque article avec les semaines pr√©c√©dentes via `ai.similarity()`\n",
    "4. **Classifie** selon les seuils :\n",
    "   - `>= 0.90` ‚Üí `is_duplicate = true`\n",
    "   - `0.85 - 0.90` ‚Üí `is_suspected_duplicate = true`\n",
    "   - `< 0.85` ‚Üí Unique\n",
    "5. **Met √† jour** la table Delta avec les r√©sultats\n",
    "\n",
    "### Pourquoi la comparaison intra-semaine ?\n",
    "\n",
    "Si 2 articles sur le **m√™me √©v√©nement** arrivent la **m√™me semaine** (ex: m√™me stade couvert par 2 journaux diff√©rents), ils doivent √™tre d√©tect√©s comme doublons entre eux, pas seulement par rapport √† l'historique.\n",
    "\n",
    "### Colonnes mises √† jour\n",
    "\n",
    "| Colonne | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| `is_duplicate` | Boolean | `true` si doublon confirm√© |\n",
    "| `is_suspected_duplicate` | Boolean | `true` si zone grise |\n",
    "| `duplicate_of` | String | ID de l'article original (peut √™tre de la m√™me semaine ou de l'historique) |\n",
    "| `duplicate_score` | Double | Score de similarit√© (0.0 √† 1.0) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}