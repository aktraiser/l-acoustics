{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Landing Feedly Opportunities\n\nPipeline de chargement des opportunites Feedly enrichies vers le Lakehouse Fabric.\n\n## Architecture Complete\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                           AZURE FUNCTIONS PIPELINE                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚  Feedly API â”€â”€â–º APIM â”€â”€â–º /api/feed â”€â”€â–º q-raw-events                        â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚                                              â–¼                              â”‚\nâ”‚                                       enrich_event                          â”‚\nâ”‚                                    (lac-weak-signals)                       â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\nâ”‚                              â–¼                               â–¼              â”‚\nâ”‚                     AI Search (upsert)              q-enriched-events       â”‚\nâ”‚                              â”‚                               â”‚              â”‚\nâ”‚                              â”‚                               â–¼              â”‚\nâ”‚                              â”‚                        analyze_event         â”‚\nâ”‚                              â”‚                     (lac-analyst-leads)      â”‚\nâ”‚                              â”‚                               â”‚              â”‚\nâ”‚                              â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\nâ”‚                              â”‚        AI Search (merge)                     â”‚\nâ”‚                              â”‚                               â”‚              â”‚\nâ”‚                              â”‚                               â–¼              â”‚\nâ”‚                              â”‚                      q-opportunities         â”‚\nâ”‚                              â”‚                      (si auditOpportunity)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â”‚\n                               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                           FABRIC LAKEHOUSE                                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚  Ce Notebook â”€â”€â–º AI Search â”€â”€â–º landing_feedly_opportunities (Delta)        â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚                                              â–¼                              â”‚\nâ”‚                                      Power BI Dashboard                     â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Modes de chargement\n\n| Mode | Table cible | Action | Quand l'utiliser |\n|------|-------------|--------|------------------|\n| `one_time` | `landing_feedly_opportunities` | OVERWRITE | Deploiement initial |\n| `incremental` | `landing_feedly_opportunities` | MERGE (upsert) | CRON hebdo |\n| `full_refresh` | `curated_feedly_opportunities` | OVERWRITE | Ad hoc / mensuel |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\n# Mode de chargement: \"one_time\", \"incremental\", \"full_refresh\"\nLOAD_MODE = \"one_time\"  # Premiere execution: charger tout l'historique\n\n# Azure AI Search\nAI_SEARCH_ENDPOINT = \"https://lac-feedly-search.search.windows.net\"\nAI_SEARCH_INDEX = \"raw_events\"\nAI_SEARCH_KEY = \"<YOUR_AI_SEARCH_KEY>\"  # TODO: Utiliser Key Vault en prod\n\n# Lakehouse Tables\nTABLE_LANDING = \"landing_feedly_opportunities\"      # Table principale (incremental)\nTABLE_CURATED = \"curated_feedly_opportunities\"      # Table consolidee (full refresh)\n\n# Incremental settings\nINCREMENTAL_HOURS = 24 * 7  # 1 semaine (168 heures)\n\nprint(f\"Mode: {LOAD_MODE}\")\nprint(f\"Table landing: {TABLE_LANDING}\")\nprint(f\"Table curated: {TABLE_CURATED}\")\nprint(f\"Incremental window: {INCREMENTAL_HOURS} heures ({INCREMENTAL_HOURS//24} jours)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports et Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, BooleanType, TimestampType, LongType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_unixtime, to_timestamp, current_timestamp,\n",
    "    lit, when, coalesce\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"âœ… Spark session ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SchÃ©ma de la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Schema unifie pour landing et curated\n# Synchronise avec le schema AI Search (recreate_index.py)\n# + colonnes ingestion_week et deduplication\nschema = StructType([\n    # Identifiant unique\n    StructField(\"id\", StringType(), False),\n    \n    # Article metadata\n    StructField(\"article_url\", StringType(), True),\n    StructField(\"article_title\", StringType(), True),\n    StructField(\"article_content\", StringType(), True),\n    StructField(\"publication_date\", TimestampType(), True),\n    StructField(\"language\", StringType(), True),\n    StructField(\"origin\", StringType(), True),\n    StructField(\"entities\", StringType(), True),\n    StructField(\"topics\", StringType(), True),\n    \n    # Vertical (market segment)\n    StructField(\"vertical\", StringType(), True),\n    \n    # Venue information\n    StructField(\"venue_name\", StringType(), True),\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"zone\", StringType(), True),  # EMEA, APAC, AMERICAS\n    StructField(\"venue_type\", StringType(), True),\n    StructField(\"capacity\", IntegerType(), True),\n    \n    # Project information\n    StructField(\"project_type\", StringType(), True),\n    StructField(\"project_phase\", StringType(), True),\n    StructField(\"opening_year\", IntegerType(), True),\n    StructField(\"opening_date\", TimestampType(), True),\n    \n    # Financial information\n    StructField(\"investment\", DoubleType(), True),\n    StructField(\"investment_currency\", StringType(), True),\n    \n    # Stakeholders\n    StructField(\"investor_owner_management\", StringType(), True),\n    StructField(\"architect_consultant_contractor\", StringType(), True),\n    \n    # Competitor intelligence\n    StructField(\"competitor_name_main\", StringType(), True),\n    StructField(\"competitor_name_other\", StringType(), True),\n    StructField(\"key_products_installed\", StringType(), True),\n    StructField(\"system_integrator\", StringType(), True),\n    StructField(\"other_key_players\", StringType(), True),\n    \n    # Additional info\n    StructField(\"additional_information\", StringType(), True),\n    \n    # Analysis results (Agent lac-analyst-leads)\n    StructField(\"evaluation_score\", IntegerType(), True),  # Score 0-100\n    StructField(\"audit_opportunity\", BooleanType(), True),\n    StructField(\"audit_opportunity_reason\", StringType(), True),\n    \n    # Metadata\n    StructField(\"crawled_at\", TimestampType(), True),  # Quand Feedly a crawle\n    StructField(\"loaded_at\", TimestampType(), True),   # Quand charge dans Fabric\n    StructField(\"updated_at\", TimestampType(), True),  # Derniere mise a jour\n    StructField(\"ingestion_week\", StringType(), True), # Semaine d'ingestion (ex: \"2024-W49\")\n    \n    # Deduplication (rempli par deduplicate_weekly.ipynb)\n    StructField(\"is_duplicate\", BooleanType(), True),           # True si doublon confirme (score >= 0.90)\n    StructField(\"is_suspected_duplicate\", BooleanType(), True), # True si zone grise (0.85 <= score < 0.90)\n    StructField(\"duplicate_of\", StringType(), True),            # ID de l'article original\n    StructField(\"duplicate_score\", DoubleType(), True)          # Score de similarite (0.0 a 1.0)\n])\n\nprint(f\"Schema defini avec {len(schema.fields)} colonnes\")\nprint(\"  - Nouvelles colonnes: ingestion_week, is_duplicate, is_suspected_duplicate, duplicate_of, duplicate_score\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions d'extraction AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_documents_from_ai_search(\n    endpoint: str, \n    index: str, \n    api_key: str, \n    mode: str = \"incremental\",\n    hours_back: int = 168,\n    batch_size: int = 1000\n):\n    \"\"\"\n    Recupere les documents depuis Azure AI Search.\n    \n    Args:\n        mode: \"one_time\" | \"incremental\" | \"full_refresh\"\n        hours_back: Pour incremental, nombre d'heures a regarder en arriere (defaut: 168 = 1 semaine)\n    \n    Note: Le filtre sur crawled utilise Int64 (millisecondes).\n    \"\"\"\n    url = f\"{endpoint}/indexes/{index}/docs/search?api-version=2023-11-01\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"api-key\": api_key\n    }\n    \n    # Construire le filtre selon le mode\n    filter_clause = None\n    if mode == \"incremental\":\n        # Filtre sur crawled (timestamp en millisecondes - Int64)\n        cutoff_ms = int((datetime.now() - timedelta(hours=hours_back)).timestamp() * 1000)\n        filter_clause = f\"crawled ge {cutoff_ms}\"\n        print(f\"Mode incremental: crawled >= {datetime.fromtimestamp(cutoff_ms/1000)} ({hours_back}h)\")\n    elif mode == \"one_time\":\n        print(\"Mode one-time: chargement de TOUT l'historique\")\n    elif mode == \"full_refresh\":\n        print(\"Mode full refresh: rechargement complet\")\n    \n    all_documents = []\n    skip = 0\n    \n    # Tous les champs du schema AI Search\n    select_fields = [\n        # Technical\n        \"id\", \"url\", \"origin\", \"published\", \"crawled\", \"language\", \"sourceId\",\n        # Content\n        \"title\", \"content\", \"entities\", \"topics\",\n        # Business Intelligence\n        \"publicationDate\", \"vertical\",\n        # Venue\n        \"venueName\", \"city\", \"country\", \"zone\", \"venueType\", \"capacity\",\n        # Project\n        \"projectType\", \"projectPhase\", \"openingYear\", \"openingDate\",\n        # Financial\n        \"investment\", \"investmentCurrency\",\n        # Stakeholders\n        \"investorOwnerManagement\", \"architectConsultantContractor\",\n        # Competitor intelligence\n        \"competitorNameMain\", \"competitorNameOther\", \"keyProductsInstalled\",\n        \"systemIntegrator\", \"otherKeyPlayers\",\n        # Additional\n        \"additionalInformation\",\n        # Analysis\n        \"evaluationScore\", \"auditOpportunity\", \"auditOpportunityReason\"\n    ]\n    \n    while True:\n        body = {\n            \"search\": \"*\",\n            \"top\": batch_size,\n            \"skip\": skip,\n            \"select\": \",\".join(select_fields),\n            \"orderby\": \"crawled desc\"\n        }\n        \n        if filter_clause:\n            body[\"filter\"] = filter_clause\n        \n        response = requests.post(url, headers=headers, json=body)\n        response.raise_for_status()\n        \n        result = response.json()\n        documents = result.get(\"value\", [])\n        \n        if not documents:\n            break\n            \n        all_documents.extend(documents)\n        skip += batch_size\n        \n        print(f\"   Fetched {len(all_documents)} documents...\")\n        \n        if len(documents) < batch_size:\n            break\n    \n    print(f\"Total: {len(all_documents)} documents recuperes\")\n    return all_documents"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_document(doc, ingestion_week: str):\n    \"\"\"\n    Transforme un document AI Search en tuple pour le DataFrame.\n    Synchronise avec le schema defini ci-dessus (38 colonnes).\n    \n    Args:\n        doc: Document depuis AI Search\n        ingestion_week: Semaine d'ingestion au format \"2024-W49\"\n    \"\"\"\n    # Publication date\n    publication_date = None\n    if doc.get(\"publicationDate\"):\n        try:\n            publication_date = datetime.fromisoformat(doc[\"publicationDate\"].replace(\"Z\", \"+00:00\"))\n        except:\n            pass\n    elif doc.get(\"published\"):\n        try:\n            publication_date = datetime.fromtimestamp(doc[\"published\"] / 1000)\n        except:\n            pass\n    \n    # Crawled date\n    crawled_at = None\n    if doc.get(\"crawled\"):\n        try:\n            crawled_at = datetime.fromtimestamp(doc[\"crawled\"] / 1000)\n        except:\n            pass\n    \n    # Opening date\n    opening_date = None\n    if doc.get(\"openingDate\"):\n        try:\n            opening_date = datetime.fromisoformat(doc[\"openingDate\"].replace(\"Z\", \"+00:00\"))\n        except:\n            pass\n    \n    now = datetime.now()\n    \n    return (\n        # id\n        doc.get(\"id\"),\n        # Article metadata\n        doc.get(\"url\", \"\"),\n        doc.get(\"title\", \"\"),\n        doc.get(\"content\", \"\"),\n        publication_date,\n        doc.get(\"language\", \"\"),\n        doc.get(\"origin\", \"\"),\n        doc.get(\"entities\", \"\"),\n        doc.get(\"topics\", \"\"),\n        # Vertical\n        doc.get(\"vertical\", \"\"),\n        # Venue\n        doc.get(\"venueName\", \"\"),\n        doc.get(\"city\", \"\"),\n        doc.get(\"country\", \"\"),\n        doc.get(\"zone\", \"\"),\n        doc.get(\"venueType\", \"\"),\n        doc.get(\"capacity\"),\n        # Project\n        doc.get(\"projectType\", \"\"),\n        doc.get(\"projectPhase\", \"\"),\n        doc.get(\"openingYear\"),\n        opening_date,\n        # Financial\n        doc.get(\"investment\"),\n        doc.get(\"investmentCurrency\", \"\"),\n        # Stakeholders\n        doc.get(\"investorOwnerManagement\", \"\"),\n        doc.get(\"architectConsultantContractor\", \"\"),\n        # Competitor intelligence\n        doc.get(\"competitorNameMain\", \"\"),\n        doc.get(\"competitorNameOther\", \"\"),\n        doc.get(\"keyProductsInstalled\", \"\"),\n        doc.get(\"systemIntegrator\", \"\"),\n        doc.get(\"otherKeyPlayers\", \"\"),\n        # Additional\n        doc.get(\"additionalInformation\", \"\"),\n        # Analysis\n        doc.get(\"evaluationScore\"),\n        doc.get(\"auditOpportunity\", False),\n        doc.get(\"auditOpportunityReason\", \"\"),\n        # Metadata\n        crawled_at,\n        now,  # loaded_at\n        now,  # updated_at\n        ingestion_week,  # ingestion_week (ex: \"2024-W49\")\n        # Deduplication (None par defaut, rempli par deduplicate_weekly.ipynb)\n        None,  # is_duplicate\n        None,  # is_suspected_duplicate\n        None,  # duplicate_of\n        None   # duplicate_score\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraction des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RÃ©cupÃ©rer les documents selon le mode\ndocuments = fetch_documents_from_ai_search(\n    endpoint=AI_SEARCH_ENDPOINT,\n    index=AI_SEARCH_INDEX,\n    api_key=AI_SEARCH_KEY,\n    mode=LOAD_MODE,\n    hours_back=INCREMENTAL_HOURS\n)\n\nif not documents:\n    print(\"âš ï¸ Aucun nouveau document Ã  charger\")\nelse:\n    # Calculer la semaine d'ingestion (format ISO 8601: \"2024-W49\")\n    ingestion_week = datetime.now().strftime(\"%Y-W%V\")\n    print(f\"Semaine d'ingestion: {ingestion_week}\")\n    \n    # Transformer les documents\n    rows = [transform_document(doc, ingestion_week) for doc in documents]\n    print(f\"âœ… {len(rows)} documents transformÃ©s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chargement selon le mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if documents:\n    # Creer le DataFrame\n    df_new = spark.createDataFrame(rows, schema)\n    print(f\"DataFrame cree: {df_new.count()} lignes\")\n    \n    # =========================================================================\n    # MODE: ONE-TIME LOAD\n    # =========================================================================\n    if LOAD_MODE == \"one_time\":\n        print(\"\\nONE-TIME LOAD: Creation initiale de la table...\")\n        \n        df_new.write \\\n            .format(\"delta\") \\\n            .mode(\"overwrite\") \\\n            .option(\"overwriteSchema\", \"true\") \\\n            .saveAsTable(TABLE_LANDING)\n        \n        print(f\"Table '{TABLE_LANDING}' creee avec {df_new.count()} enregistrements\")\n    \n    # =========================================================================\n    # MODE: INCREMENTAL (DELTA)\n    # =========================================================================\n    elif LOAD_MODE == \"incremental\":\n        print(\"\\nINCREMENTAL: Merge des nouveaux documents...\")\n        \n        # Verifier si la table existe\n        table_exists = spark.catalog.tableExists(TABLE_LANDING)\n        \n        if not table_exists:\n            print(f\"   Table '{TABLE_LANDING}' n'existe pas, creation...\")\n            df_new.write \\\n                .format(\"delta\") \\\n                .mode(\"overwrite\") \\\n                .saveAsTable(TABLE_LANDING)\n        else:\n            # MERGE: Insert new, Update existing\n            # Note: ingestion_week et colonnes deduplication ne sont PAS mises a jour\n            #       - ingestion_week: conserve la semaine d'origine\n            #       - is_duplicate, is_suspected_duplicate, duplicate_of, duplicate_score:\n            #         geres par deduplicate_weekly.ipynb\n            delta_table = DeltaTable.forName(spark, TABLE_LANDING)\n            \n            delta_table.alias(\"target\").merge(\n                df_new.alias(\"source\"),\n                \"target.id = source.id\"\n            ).whenMatchedUpdate(\n                set={\n                    \"article_url\": \"source.article_url\",\n                    \"article_title\": \"source.article_title\",\n                    \"article_content\": \"source.article_content\",\n                    \"publication_date\": \"source.publication_date\",\n                    \"language\": \"source.language\",\n                    \"origin\": \"source.origin\",\n                    \"entities\": \"source.entities\",\n                    \"topics\": \"source.topics\",\n                    \"vertical\": \"source.vertical\",\n                    \"venue_name\": \"source.venue_name\",\n                    \"city\": \"source.city\",\n                    \"country\": \"source.country\",\n                    \"zone\": \"source.zone\",\n                    \"venue_type\": \"source.venue_type\",\n                    \"capacity\": \"source.capacity\",\n                    \"project_type\": \"source.project_type\",\n                    \"project_phase\": \"source.project_phase\",\n                    \"opening_year\": \"source.opening_year\",\n                    \"opening_date\": \"source.opening_date\",\n                    \"investment\": \"source.investment\",\n                    \"investment_currency\": \"source.investment_currency\",\n                    \"investor_owner_management\": \"source.investor_owner_management\",\n                    \"architect_consultant_contractor\": \"source.architect_consultant_contractor\",\n                    \"competitor_name_main\": \"source.competitor_name_main\",\n                    \"competitor_name_other\": \"source.competitor_name_other\",\n                    \"key_products_installed\": \"source.key_products_installed\",\n                    \"system_integrator\": \"source.system_integrator\",\n                    \"other_key_players\": \"source.other_key_players\",\n                    \"additional_information\": \"source.additional_information\",\n                    \"evaluation_score\": \"source.evaluation_score\",\n                    \"audit_opportunity\": \"source.audit_opportunity\",\n                    \"audit_opportunity_reason\": \"source.audit_opportunity_reason\",\n                    \"crawled_at\": \"source.crawled_at\",\n                    \"updated_at\": \"source.updated_at\"\n                    # Ne PAS mettre a jour:\n                    # - loaded_at (date de premier chargement)\n                    # - ingestion_week (semaine d'origine)\n                    # - is_duplicate, is_suspected_duplicate, duplicate_of, duplicate_score\n                    #   (geres par deduplicate_weekly.ipynb)\n                }\n            ).whenNotMatchedInsertAll().execute()\n            \n            print(f\"Merge termine\")\n        \n        # Stats\n        total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_LANDING}\").collect()[0][0]\n        print(f\"   Total dans la table: {total} enregistrements\")\n    \n    # =========================================================================\n    # MODE: FULL REFRESH\n    # =========================================================================\n    elif LOAD_MODE == \"full_refresh\":\n        print(\"\\nFULL REFRESH: Rechargement complet...\")\n        \n        # Ecraser la table curated (pas la landing)\n        df_new.write \\\n            .format(\"delta\") \\\n            .mode(\"overwrite\") \\\n            .option(\"overwriteSchema\", \"true\") \\\n            .saveAsTable(TABLE_CURATED)\n        \n        print(f\"Table '{TABLE_CURATED}' recreee avec {df_new.count()} enregistrements\")\n        \n        # Vacuum pour nettoyer les anciens fichiers\n        spark.sql(f\"VACUUM {TABLE_CURATED} RETAIN 168 HOURS\")\n        print(\"   Vacuum effectue\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation et Statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©terminer quelle table afficher\n",
    "display_table = TABLE_CURATED if LOAD_MODE == \"full_refresh\" else TABLE_LANDING\n",
    "\n",
    "print(f\"ğŸ“Š Statistiques de '{display_table}':\\n\")\n",
    "\n",
    "# Counts\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {display_table}\").collect()[0][0]\n",
    "opportunities = spark.sql(f\"SELECT COUNT(*) FROM {display_table} WHERE audit_opportunity = true\").collect()[0][0]\n",
    "avg_score = spark.sql(f\"SELECT AVG(evaluation_score) FROM {display_table} WHERE evaluation_score IS NOT NULL\").collect()[0][0]\n",
    "\n",
    "print(f\"   ğŸ“ˆ Total records: {total}\")\n",
    "print(f\"   ğŸ¯ Opportunities (audit=true): {opportunities} ({opportunities/total*100:.1f}%)\" if total > 0 else \"   ğŸ¯ Opportunities: 0\")\n",
    "print(f\"   ğŸ“Š Average score: {avg_score:.1f}\" if avg_score else \"   ğŸ“Š Average score: N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par Vertical\n",
    "print(\"\\nğŸ“Š Distribution par Vertical:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        vertical,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN audit_opportunity = true THEN 1 ELSE 0 END) as opportunities,\n",
    "        ROUND(AVG(evaluation_score), 1) as avg_score\n",
    "    FROM {display_table}\n",
    "    WHERE vertical IS NOT NULL AND vertical != ''\n",
    "    GROUP BY vertical\n",
    "    ORDER BY total DESC\n",
    "\"\"\").show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par Zone\n",
    "print(\"\\nğŸŒ Distribution par Zone:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        zone,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN audit_opportunity = true THEN 1 ELSE 0 END) as opportunities,\n",
    "        ROUND(AVG(evaluation_score), 1) as avg_score\n",
    "    FROM {display_table}\n",
    "    WHERE zone IS NOT NULL AND zone != ''\n",
    "    GROUP BY zone\n",
    "    ORDER BY total DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par Project Phase\n",
    "print(\"\\nğŸ—ï¸ Distribution par Project Phase:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        project_phase,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN audit_opportunity = true THEN 1 ELSE 0 END) as opportunities,\n",
    "        ROUND(AVG(evaluation_score), 1) as avg_score\n",
    "    FROM {display_table}\n",
    "    WHERE project_phase IS NOT NULL AND project_phase != ''\n",
    "    GROUP BY project_phase\n",
    "    ORDER BY total DESC\n",
    "\"\"\").show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 OpportunitÃ©s\n",
    "print(\"\\nğŸ† Top 10 OpportunitÃ©s (score le plus Ã©levÃ©):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        venue_name,\n",
    "        city,\n",
    "        country,\n",
    "        vertical,\n",
    "        project_phase,\n",
    "        evaluation_score,\n",
    "        opening_year\n",
    "    FROM {display_table}\n",
    "    WHERE audit_opportunity = true\n",
    "    ORDER BY evaluation_score DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AperÃ§u des derniers chargements\n",
    "print(\"\\nğŸ• Derniers enregistrements chargÃ©s:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        article_title,\n",
    "        vertical,\n",
    "        city,\n",
    "        country,\n",
    "        evaluation_score,\n",
    "        audit_opportunity,\n",
    "        loaded_at\n",
    "    FROM {display_table}\n",
    "    ORDER BY loaded_at DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Historique Delta (debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historique des opÃ©rations Delta\n",
    "if spark.catalog.tableExists(TABLE_LANDING):\n",
    "    print(f\"\\nğŸ“œ Historique Delta de '{TABLE_LANDING}':\")\n",
    "    spark.sql(f\"DESCRIBE HISTORY {TABLE_LANDING}\").select(\n",
    "        \"version\", \"timestamp\", \"operation\", \"operationParameters\"\n",
    "    ).show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Pipeline complet\n\n**Chaque semaine:**\n\n1. **Azure Function Collector** `/api/feed` â†’ Feedly â†’ `q-raw-events`\n2. **enrich_event** â†’ Agent `lac-weak-signals` â†’ AI Search (upsert) â†’ `q-enriched-events`\n3. **analyze_event** â†’ Agent `lac-analyst-leads` â†’ AI Search (merge) â†’ `q-opportunities` (si opportunite)\n4. **Ce Notebook Fabric** (`LOAD_MODE=\"one_time\"` au deploiement, puis `\"incremental\"` en CRON hebdo)\n   â†’ AI Search â†’ Lakehouse `landing_feedly_opportunities`\n5. **deduplicate_weekly.ipynb** â†’ Deduplication semantique avec Fabric AI Functions\n6. **Power BI / Fabric** pour revue metier & priorisation\n\n---\n\n## Resume des modes\n\n| Mode | Table cible | Action | Quand l'utiliser |\n|------|-------------|--------|------------------|\n| `one_time` | `landing_feedly_opportunities` | OVERWRITE | Deploiement initial |\n| `incremental` | `landing_feedly_opportunities` | MERGE (upsert) | CRON hebdo |\n| `full_refresh` | `curated_feedly_opportunities` | OVERWRITE | Ad hoc / mensuel |\n\n---\n\n## Colonnes de la table (38 colonnes)\n\n| Categorie | Colonnes |\n|-----------|----------|\n| **ID** | `id` |\n| **Article** | `article_url`, `article_title`, `article_content`, `publication_date`, `language`, `origin`, `entities`, `topics` |\n| **Vertical** | `vertical` |\n| **Venue** | `venue_name`, `city`, `country`, `zone`, `venue_type`, `capacity` |\n| **Projet** | `project_type`, `project_phase`, `opening_year`, `opening_date` |\n| **Financier** | `investment`, `investment_currency` |\n| **Stakeholders** | `investor_owner_management`, `architect_consultant_contractor` |\n| **Concurrents** | `competitor_name_main`, `competitor_name_other`, `key_products_installed`, `system_integrator`, `other_key_players` |\n| **Additional** | `additional_information` |\n| **Analyse** | `evaluation_score`, `audit_opportunity`, `audit_opportunity_reason` |\n| **Metadata** | `crawled_at`, `loaded_at`, `updated_at`, `ingestion_week` |\n| **Deduplication** | `is_duplicate`, `is_suspected_duplicate`, `duplicate_of`, `duplicate_score` |\n\n---\n\n## Deduplication\n\nLes colonnes de deduplication sont **gerees par un notebook separe** (`deduplicate_weekly.ipynb`) qui utilise les Fabric AI Functions:\n\n| Colonne | Type | Description |\n|---------|------|-------------|\n| `ingestion_week` | String | Semaine d'ingestion (ex: \"2024-W49\") |\n| `is_duplicate` | Boolean | `true` si doublon confirme (score >= 0.90) |\n| `is_suspected_duplicate` | Boolean | `true` si zone grise (0.85 <= score < 0.90) |\n| `duplicate_of` | String | ID de l'article original (si doublon) |\n| `duplicate_score` | Double | Score de similarite (0.0 a 1.0) |\n\n**Texte compare pour embedding:** `title + venue_name + city + country`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}