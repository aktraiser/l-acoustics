{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Landing Salers\n\nPipeline de chargement des données commerciaux (Excel) vers le Lakehouse Fabric.\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                           FABRIC LAKEHOUSE                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Excel \"Contacts RAE RSM BD for Leads.xlsx\" (3 feuilles):                  │\n│                                                                             │\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\n│  │ RAE_RSM + emails  ──►  landing_salers (par PAYS)                    │   │\n│  │   - Assignation par pays                                            │   │\n│  │   - JOIN opportunites sur country                                   │   │\n│  │   - Voit TOUTES les verticales de son pays                          │   │\n│  └─────────────────────────────────────────────────────────────────────┘   │\n│                                                                             │\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\n│  │ BD + emails  ──►  landing_salers_bd (par VERTICALE)                 │   │\n│  │   - Assignation par market_segment                                  │   │\n│  │   - JOIN opportunites sur market_segment                            │   │\n│  │   - Voit TOUS les pays de sa verticale                              │   │\n│  └─────────────────────────────────────────────────────────────────────┘   │\n│                                                                             │\n│  Note: 2 profils hybrides (Zohar Pajela, Andre Pichette) sont dans les 2  │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\n# Mode de chargement: \"one_time\", \"incremental\", \"full_refresh\"\nLOAD_MODE = \"one_time\"  # Premiere execution: charger tout\n\n# Chemin vers le fichier Excel dans le Lakehouse Fabric\nEXCEL_PATH = \"/lakehouse/default/Files/\"\nEXCEL_FILENAME = \"Contacts RAE RSM BD for Leads.xlsx\"\n\n# Noms des feuilles Excel\nSHEET_RAE_RSM = \"RAE_RSM\"    # Feuille des commerciaux (RAE + RSM) - par pays\nSHEET_EMAILS = \"emails\"       # Feuille des emails (Name, Email)\nSHEET_BD = \"BD\"               # Feuille BD - par verticale (Market segment)\n\n# Lakehouse Tables\nTABLE_SALERS = \"landing_salers\"        # RAE/RSM par pays\nTABLE_SALERS_BD = \"landing_salers_bd\"  # BD par verticale\n\nprint(f\"Mode: {LOAD_MODE}\")\nprint(f\"Excel: {EXCEL_PATH}{EXCEL_FILENAME}\")\nprint(f\"Feuilles: {SHEET_RAE_RSM}, {SHEET_EMAILS}, {SHEET_BD}\")\nprint(f\"Tables cibles: {TABLE_SALERS} (par pays), {TABLE_SALERS_BD} (par verticale)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports et Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, lit, trim, upper, lower,\n",
    "    regexp_replace, when\n",
    ")\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark session ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema de la table Salers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Schema pour RAE/RSM (par pays) - sans market_segment\nschema_salers = StructType([\n    StructField(\"sales_zone\", StringType(), True),      # EMEA, APAC, AMERICAS\n    StructField(\"sales_region\", StringType(), True),    # Region de vente\n    StructField(\"country_code\", StringType(), True),    # Code pays (FR, US, etc.)\n    StructField(\"country\", StringType(), True),         # Nom du pays\n    StructField(\"who\", StringType(), True),             # Nom du commercial\n    StructField(\"role\", StringType(), True),            # Role (RAE, RSM)\n    StructField(\"email\", StringType(), True),           # Email du commercial\n    # Metadata\n    StructField(\"loaded_at\", TimestampType(), True),\n    StructField(\"updated_at\", TimestampType(), True),\n])\n\n# Schema pour BD (par verticale)\nschema_salers_bd = StructType([\n    StructField(\"market_segment\", StringType(), True),  # Verticale (Live Events, Install...)\n    StructField(\"who\", StringType(), True),             # Nom du BD\n    StructField(\"role\", StringType(), True),            # Role (BD)\n    StructField(\"email\", StringType(), True),           # Email\n    # Metadata\n    StructField(\"loaded_at\", TimestampType(), True),\n    StructField(\"updated_at\", TimestampType(), True),\n])\n\nprint(f\"Schema landing_salers: {len(schema_salers.fields)} colonnes (par pays)\")\nprint(f\"Schema landing_salers_bd: {len(schema_salers_bd.fields)} colonnes (par verticale)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lecture du fichier Excel (3 feuilles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Construire le chemin complet\nexcel_full_path = f\"{EXCEL_PATH}{EXCEL_FILENAME}\"\n\nprint(f\"Lecture du fichier: {excel_full_path}\")\n\n# 1. Lire la feuille RAE_RSM (commerciaux)\nprint(f\"\\n1. Lecture feuille '{SHEET_RAE_RSM}'...\")\ndf_rae_rsm = pd.read_excel(excel_full_path, sheet_name=SHEET_RAE_RSM)\nprint(f\"   {len(df_rae_rsm)} lignes chargees\")\nprint(f\"   Colonnes: {list(df_rae_rsm.columns)}\")\n\n# 2. Lire la feuille emails\nprint(f\"\\n2. Lecture feuille '{SHEET_EMAILS}'...\")\ndf_emails = pd.read_excel(excel_full_path, sheet_name=SHEET_EMAILS)\nprint(f\"   {len(df_emails)} lignes chargees\")\nprint(f\"   Colonnes: {list(df_emails.columns)}\")\n\n# 3. Lire la feuille BD (Market segment)\nprint(f\"\\n3. Lecture feuille '{SHEET_BD}'...\")\ndf_bd = pd.read_excel(excel_full_path, sheet_name=SHEET_BD)\nprint(f\"   {len(df_bd)} lignes chargees\")\nprint(f\"   Colonnes: {list(df_bd.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apercu des donnees RAE_RSM\nprint(\"Apercu RAE_RSM:\")\ndf_rae_rsm.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apercu des donnees BD (Market segment)\nprint(\"Apercu BD (Market segment):\")\ndf_bd.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Jointure emails (Who = Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trouver le nom de la colonne email dans df_emails\nemail_col = None\nfor c in df_emails.columns:\n    if 'mail' in c.lower():\n        email_col = c\n        break\n\nname_col = None\nfor c in df_emails.columns:\n    if c.lower() in ['name', 'nom', 'who']:\n        name_col = c\n        break\n\nprint(f\"Colonne Name detectee: '{name_col}'\")\nprint(f\"Colonne Email detectee: '{email_col}'\")\n\n# Renommer et preparer pour la jointure\ndf_emails_clean = df_emails[[name_col, email_col]].copy()\ndf_emails_clean.columns = ['Who', 'email']\ndf_emails_clean = df_emails_clean.drop_duplicates(subset=['Who'], keep='first')\n\n# JOIN RAE_RSM avec emails\ndf_with_email = df_rae_rsm.merge(df_emails_clean, on='Who', how='left')\nprint(f\"\\nApres jointure emails: {len(df_with_email)} lignes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Preparation des emails (normalises)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preparer la table emails avec normalisation pour jointures\ndf_emails_clean = df_emails[[name_col, email_col]].copy()\ndf_emails_clean.columns = ['Name', 'email']\ndf_emails_clean['Name_normalized'] = df_emails_clean['Name'].str.strip().str.lower()\ndf_emails_clean = df_emails_clean.drop_duplicates(subset=['Name_normalized'], keep='first')\n\nprint(f\"{len(df_emails_clean)} emails uniques prepares\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Table 1: landing_salers (RAE/RSM par pays)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RAE_RSM + emails (jointure sur Who = Name)\ndf_rae_rsm['Who_normalized'] = df_rae_rsm['Who'].str.strip().str.lower()\n\ndf_salers = df_rae_rsm.merge(\n    df_emails_clean[['Name_normalized', 'email']],\n    left_on='Who_normalized',\n    right_on='Name_normalized',\n    how='left'\n)\n\n# Nettoyer colonnes temporaires\ndf_salers = df_salers.drop(columns=['Who_normalized', 'Name_normalized'], errors='ignore')\n\nprint(f\"landing_salers: {len(df_salers)} lignes\")\nprint(f\"Avec email: {df_salers['email'].notna().sum()}\")\nprint(f\"\\nApercu:\")\ndf_salers.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Table 2: landing_salers_bd (BD par verticale)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# BD + emails (jointure sur Who = Name)\ndf_bd['Who_normalized'] = df_bd['Who'].str.strip().str.lower()\n\ndf_salers_bd = df_bd.merge(\n    df_emails_clean[['Name_normalized', 'email']],\n    left_on='Who_normalized',\n    right_on='Name_normalized',\n    how='left'\n)\n\n# Nettoyer colonnes temporaires\ndf_salers_bd = df_salers_bd.drop(columns=['Who_normalized', 'Name_normalized'], errors='ignore')\n\nprint(f\"landing_salers_bd: {len(df_salers_bd)} lignes\")\nprint(f\"Avec email: {df_salers_bd['email'].notna().sum()}\")\nprint(f\"\\nDistribution par Market segment:\")\nprint(df_salers_bd['Market segment'].value_counts().to_string())\nprint(f\"\\nApercu:\")\ndf_salers_bd.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 9. Conversion en Spark DataFrames"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Convertir en Spark DataFrames\nsdf_salers_raw = spark.createDataFrame(df_salers)\nsdf_salers_bd_raw = spark.createDataFrame(df_salers_bd)\n\nprint(f\"Spark DataFrame landing_salers: {sdf_salers_raw.count()} lignes\")\nprint(f\"Spark DataFrame landing_salers_bd: {sdf_salers_bd_raw.count()} lignes\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 10. Transformation des donnees"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verifier que les DataFrames existent, sinon les recreer\ntry:\n    sdf_salers_raw\nexcept NameError:\n    print(\"Recreation des DataFrames Spark...\")\n    sdf_salers_raw = spark.createDataFrame(df_salers)\n    sdf_salers_bd_raw = spark.createDataFrame(df_salers_bd)\n\n# Transformation landing_salers (RAE/RSM par pays)\ndef transform_salers(df):\n    df_renamed = df \\\n        .withColumnRenamed(\"Sales Zone\", \"sales_zone\") \\\n        .withColumnRenamed(\"Sales Region\", \"sales_region\") \\\n        .withColumnRenamed(\"Code\", \"country_code\") \\\n        .withColumnRenamed(\"Country\", \"country\") \\\n        .withColumnRenamed(\"Who\", \"who\") \\\n        .withColumnRenamed(\"Role\", \"role\")\n    \n    df_clean = df_renamed \\\n        .withColumn(\"sales_zone\", upper(trim(col(\"sales_zone\")))) \\\n        .withColumn(\"sales_region\", trim(col(\"sales_region\"))) \\\n        .withColumn(\"country_code\", upper(trim(col(\"country_code\")))) \\\n        .withColumn(\"country\", trim(col(\"country\"))) \\\n        .withColumn(\"who\", trim(col(\"who\"))) \\\n        .withColumn(\"role\", upper(trim(col(\"role\")))) \\\n        .withColumn(\"email\", lower(trim(col(\"email\"))))\n    \n    df_filtered = df_clean.filter(col(\"country_code\").isNotNull() & (col(\"country_code\") != \"\"))\n    \n    return df_filtered \\\n        .withColumn(\"loaded_at\", current_timestamp()) \\\n        .withColumn(\"updated_at\", current_timestamp()) \\\n        .select(\"sales_zone\", \"sales_region\", \"country_code\", \"country\", \"who\", \"role\", \"email\", \"loaded_at\", \"updated_at\")\n\n# Transformation landing_salers_bd (BD par verticale)\ndef transform_salers_bd(df):\n    df_renamed = df \\\n        .withColumnRenamed(\"Market segment\", \"market_segment\") \\\n        .withColumnRenamed(\"Who\", \"who\") \\\n        .withColumnRenamed(\"Role\", \"role\")\n    \n    df_clean = df_renamed \\\n        .withColumn(\"market_segment\", trim(col(\"market_segment\"))) \\\n        .withColumn(\"who\", trim(col(\"who\"))) \\\n        .withColumn(\"role\", upper(trim(col(\"role\")))) \\\n        .withColumn(\"email\", lower(trim(col(\"email\"))))\n    \n    df_filtered = df_clean.filter(col(\"market_segment\").isNotNull() & (col(\"market_segment\") != \"\"))\n    \n    return df_filtered \\\n        .withColumn(\"loaded_at\", current_timestamp()) \\\n        .withColumn(\"updated_at\", current_timestamp()) \\\n        .select(\"market_segment\", \"who\", \"role\", \"email\", \"loaded_at\", \"updated_at\")\n\n# Appliquer transformations\nsdf_salers = transform_salers(sdf_salers_raw)\nsdf_salers_bd = transform_salers_bd(sdf_salers_bd_raw)\n\nprint(f\"landing_salers transforme: {sdf_salers.count()} lignes\")\nsdf_salers.printSchema()\n\nprint(f\"\\nlanding_salers_bd transforme: {sdf_salers_bd.count()} lignes\")\nsdf_salers_bd.printSchema()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apercu des donnees transformees\nprint(\"Apercu landing_salers (par pays):\")\nsdf_salers.show(10, truncate=False)\n\nprint(\"\\nApercu landing_salers_bd (par verticale):\")\nsdf_salers_bd.show(10, truncate=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 11. Chargement des tables Delta"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# CHARGEMENT TABLE 1: landing_salers (RAE/RSM par pays)\n# =============================================================================\nprint(f\"=== Chargement {TABLE_SALERS} ===\\n\")\n\nif LOAD_MODE == \"one_time\":\n    sdf_salers.write \\\n        .format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .option(\"overwriteSchema\", \"true\") \\\n        .saveAsTable(TABLE_SALERS)\n    print(f\"Table '{TABLE_SALERS}' creee avec {sdf_salers.count()} enregistrements\")\n\nelif LOAD_MODE == \"incremental\":\n    if not spark.catalog.tableExists(TABLE_SALERS):\n        sdf_salers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_SALERS)\n    else:\n        delta_table = DeltaTable.forName(spark, TABLE_SALERS)\n        delta_table.alias(\"target\").merge(\n            sdf_salers.alias(\"source\"),\n            \"target.country_code = source.country_code AND target.who = source.who\"\n        ).whenMatchedUpdate(set={\n            \"sales_zone\": \"source.sales_zone\",\n            \"sales_region\": \"source.sales_region\",\n            \"country\": \"source.country\",\n            \"role\": \"source.role\",\n            \"email\": \"source.email\",\n            \"updated_at\": \"source.updated_at\"\n        }).whenNotMatchedInsertAll().execute()\n    total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_SALERS}\").collect()[0][0]\n    print(f\"Table '{TABLE_SALERS}': {total} enregistrements\")\n\nelif LOAD_MODE == \"full_refresh\":\n    sdf_salers.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TABLE_SALERS)\n    spark.sql(f\"VACUUM {TABLE_SALERS} RETAIN 168 HOURS\")\n    print(f\"Table '{TABLE_SALERS}' recreee avec {sdf_salers.count()} enregistrements\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CHARGEMENT TABLE 2: landing_salers_bd (BD par verticale)\n# =============================================================================\nprint(f\"=== Chargement {TABLE_SALERS_BD} ===\\n\")\n\nif LOAD_MODE == \"one_time\":\n    sdf_salers_bd.write \\\n        .format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .option(\"overwriteSchema\", \"true\") \\\n        .saveAsTable(TABLE_SALERS_BD)\n    print(f\"Table '{TABLE_SALERS_BD}' creee avec {sdf_salers_bd.count()} enregistrements\")\n\nelif LOAD_MODE == \"incremental\":\n    if not spark.catalog.tableExists(TABLE_SALERS_BD):\n        sdf_salers_bd.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TABLE_SALERS_BD)\n    else:\n        delta_table = DeltaTable.forName(spark, TABLE_SALERS_BD)\n        delta_table.alias(\"target\").merge(\n            sdf_salers_bd.alias(\"source\"),\n            \"target.market_segment = source.market_segment AND target.who = source.who\"\n        ).whenMatchedUpdate(set={\n            \"role\": \"source.role\",\n            \"email\": \"source.email\",\n            \"updated_at\": \"source.updated_at\"\n        }).whenNotMatchedInsertAll().execute()\n    total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_SALERS_BD}\").collect()[0][0]\n    print(f\"Table '{TABLE_SALERS_BD}': {total} enregistrements\")\n\nelif LOAD_MODE == \"full_refresh\":\n    sdf_salers_bd.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TABLE_SALERS_BD)\n    spark.sql(f\"VACUUM {TABLE_SALERS_BD} RETAIN 168 HOURS\")\n    print(f\"Table '{TABLE_SALERS_BD}' recreee avec {sdf_salers_bd.count()} enregistrements\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 12. Validation et Statistiques"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# STATISTIQUES TABLE 1: landing_salers (par pays)\n# =============================================================================\nprint(f\"=== Statistiques {TABLE_SALERS} (par pays) ===\\n\")\n\ntotal = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_SALERS}\").collect()[0][0]\nzones = spark.sql(f\"SELECT COUNT(DISTINCT sales_zone) FROM {TABLE_SALERS}\").collect()[0][0]\ncountries = spark.sql(f\"SELECT COUNT(DISTINCT country) FROM {TABLE_SALERS}\").collect()[0][0]\npersons = spark.sql(f\"SELECT COUNT(DISTINCT who) FROM {TABLE_SALERS}\").collect()[0][0]\nwith_email = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_SALERS} WHERE email IS NOT NULL AND email != ''\").collect()[0][0]\n\nprint(f\"Total lignes: {total}\")\nprint(f\"Zones distinctes: {zones}\")\nprint(f\"Pays distincts: {countries}\")\nprint(f\"Personnes distinctes: {persons}\")\nprint(f\"Avec email: {with_email} ({with_email/total*100:.1f}%)\")\n\nprint(\"\\nDistribution par Zone:\")\nspark.sql(f\"\"\"\n    SELECT sales_zone, COUNT(*) as total, COUNT(DISTINCT country) as nb_pays, COUNT(DISTINCT who) as nb_personnes\n    FROM {TABLE_SALERS}\n    GROUP BY sales_zone ORDER BY total DESC\n\"\"\").show(truncate=False)\n\nprint(\"Distribution par Role:\")\nspark.sql(f\"\"\"\n    SELECT role, COUNT(*) as total, COUNT(DISTINCT who) as nb_personnes\n    FROM {TABLE_SALERS}\n    GROUP BY role ORDER BY total DESC\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# STATISTIQUES TABLE 2: landing_salers_bd (par verticale)\n# =============================================================================\nprint(f\"=== Statistiques {TABLE_SALERS_BD} (par verticale) ===\\n\")\n\ntotal_bd = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_SALERS_BD}\").collect()[0][0]\nsegments = spark.sql(f\"SELECT COUNT(DISTINCT market_segment) FROM {TABLE_SALERS_BD}\").collect()[0][0]\npersons_bd = spark.sql(f\"SELECT COUNT(DISTINCT who) FROM {TABLE_SALERS_BD}\").collect()[0][0]\nwith_email_bd = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_SALERS_BD} WHERE email IS NOT NULL AND email != ''\").collect()[0][0]\n\nprint(f\"Total lignes: {total_bd}\")\nprint(f\"Market segments distincts: {segments}\")\nprint(f\"Personnes distinctes: {persons_bd}\")\nprint(f\"Avec email: {with_email_bd} ({with_email_bd/total_bd*100:.1f}%)\")\n\nprint(\"\\nDistribution par Market Segment:\")\nspark.sql(f\"\"\"\n    SELECT market_segment, COUNT(*) as total, COUNT(DISTINCT who) as nb_personnes\n    FROM {TABLE_SALERS_BD}\n    GROUP BY market_segment ORDER BY total DESC\n\"\"\").show(truncate=False)\n\nprint(\"Liste des BD:\")\nspark.sql(f\"\"\"\n    SELECT market_segment, who, email\n    FROM {TABLE_SALERS_BD}\n    ORDER BY market_segment, who\n\"\"\").show(50, truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identifier les profils hybrides (present dans les 2 tables)\nprint(\"=== Profils hybrides (dans les 2 tables) ===\\n\")\nspark.sql(f\"\"\"\n    SELECT s.who, s.country, s.role as role_pays, bd.market_segment, bd.role as role_verticale\n    FROM {TABLE_SALERS} s\n    INNER JOIN {TABLE_SALERS_BD} bd ON LOWER(TRIM(s.who)) = LOWER(TRIM(bd.who))\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. Historique Delta"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Historique Delta de '{TABLE_SALERS}':\")\nif spark.catalog.tableExists(TABLE_SALERS):\n    spark.sql(f\"DESCRIBE HISTORY {TABLE_SALERS}\").select(\"version\", \"timestamp\", \"operation\").show(5, truncate=50)\n\nprint(f\"\\nHistorique Delta de '{TABLE_SALERS_BD}':\")\nif spark.catalog.tableExists(TABLE_SALERS_BD):\n    spark.sql(f\"DESCRIBE HISTORY {TABLE_SALERS_BD}\").select(\"version\", \"timestamp\", \"operation\").show(5, truncate=50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Schema des tables\n\n### landing_salers (par pays)\n| Colonne | Type | Description |\n|---------|------|-------------|\n| `sales_zone` | String | Zone (EMEA, APAC, AMERICAS) |\n| `sales_region` | String | Region de vente |\n| `country_code` | String | Code pays (FR, US...) |\n| `country` | String | Nom du pays |\n| `who` | String | Nom du commercial |\n| `role` | String | Role (RAE, RSM) |\n| `email` | String | Email du commercial |\n| `loaded_at` | Timestamp | Date de chargement |\n| `updated_at` | Timestamp | Date de mise a jour |\n\n**Usage Warehouse**: JOIN sur `country` avec les opportunites\n\n### landing_salers_bd (par verticale)\n| Colonne | Type | Description |\n|---------|------|-------------|\n| `market_segment` | String | Verticale (Live Events, Install...) |\n| `who` | String | Nom du BD |\n| `role` | String | Role (BD) |\n| `email` | String | Email |\n| `loaded_at` | Timestamp | Date de chargement |\n| `updated_at` | Timestamp | Date de mise a jour |\n\n**Usage Warehouse**: JOIN sur `market_segment` avec les opportunites"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}